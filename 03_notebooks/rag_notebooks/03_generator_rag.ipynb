{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c8c2f6",
   "metadata": {},
   "source": [
    "**Notebook 03 is a \"run-once\" setup**\n",
    "\n",
    "- üìù NOTEBOOK 3 - SETUP ONLY\n",
    "- ‚úÖ LLM client configured\n",
    "- ‚úÖ Prompt templates defined  \n",
    "- ‚úÖ Answer generator ready\n",
    "\n",
    "No files saved - this notebook only needs to run once per session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93446b4",
   "metadata": {},
   "source": [
    "# LLM Response Generation\n",
    "\n",
    "**Why we're doing this:**\n",
    " Take retrieved document chunks and generate coherent answers using a language model.\n",
    "\n",
    "**What we're doing:**\n",
    "\n",
    "- Setting up first prototype - done\n",
    "- Setting up the LLM client (Groq/Llama) - done\n",
    "- Creating prompt templates for TRL questions - done\n",
    "- Generating answers from retrieved context - done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a9148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ COMPONENTS IMPORTED SUCCESSFULLY!\n",
      "‚úì TF-IDF retriever loaded successfully\n",
      "‚úì Template-based RAG answer generator initialized\n",
      "‚úÖ Generation pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "# PERMANENT WORKING IMPORT - USE THIS EVERYWHERE\n",
    "import sys\n",
    "import os\n",
    "import importlib.util\n",
    "\n",
    "def import_rag_components():\n",
    "    \"\"\"Import RAG components\"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Import retriever\n",
    "    retriever_path = os.path.join(current_dir, 'rag_components', 'retriever.py')\n",
    "    spec = importlib.util.spec_from_file_location(\"retriever\", retriever_path)\n",
    "    retriever_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(retriever_module)\n",
    "    \n",
    "    # Import query_interface  \n",
    "    query_interface_path = os.path.join(current_dir, 'rag_components', 'query_interface.py')\n",
    "    spec = importlib.util.spec_from_file_location(\"query_interface\", query_interface_path)\n",
    "    query_interface_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(query_interface_module)\n",
    "    \n",
    "    # Import answer_generator\n",
    "    answer_generator_path = os.path.join(current_dir, 'rag_components', 'answer_generator.py')\n",
    "    spec = importlib.util.spec_from_file_location(\"answer_generator\", answer_generator_path)\n",
    "    answer_generator_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(answer_generator_module)\n",
    "    \n",
    "    return (retriever_module.DocumentAwareRetriever, \n",
    "            query_interface_module.SimpleQueryInterface,\n",
    "            answer_generator_module.RAGAnswerGenerator)\n",
    "\n",
    "# Import the components\n",
    "DocumentAwareRetriever, SimpleQueryInterface, RAGAnswerGenerator = import_rag_components()\n",
    "print(\"üéâ COMPONENTS IMPORTED SUCCESSFULLY!\")\n",
    "\n",
    "# Continue with code\n",
    "VECTOR_INDEX_PATH = \"../../04_models/vector_index\"\n",
    "retriever = DocumentAwareRetriever(VECTOR_INDEX_PATH)\n",
    "query_interface = SimpleQueryInterface(retriever)\n",
    "answer_generator = RAGAnswerGenerator(query_interface)\n",
    "print(\"‚úÖ Generation pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b256e76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from groq) (4.10.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from groq) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/siriamandaraaf/.pyenv/versions/3.11.3/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
      "Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, groq\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [groq]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 groq-0.36.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b686a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Groq client initialized successfully\n",
      "üéâ LLM client ready for integration!\n"
     ]
    }
   ],
   "source": [
    "# CELL: LLM Client Setup\n",
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client\n",
    "def setup_groq_client():\n",
    "    \"\"\"Set up and return Groq client with error handling\"\"\"\n",
    "    api_key = os.getenv('GROQ_API_KEY')\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå GROQ_API_KEY not found in environment variables\")\n",
    "    \n",
    "    client = Groq(api_key=api_key)\n",
    "    print(\"‚úÖ Groq client initialized successfully\")\n",
    "    return client\n",
    "\n",
    "# Test the client\n",
    "try:\n",
    "    groq_client = setup_groq_client()\n",
    "    print(\"üéâ LLM client ready for integration!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize LLM client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962956a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Connected: API connected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL: Test LLM Connection\n",
    "# Why: Verify Groq API works and model responds correctly\n",
    "# What: Send simple test query to confirm setup is functional\n",
    "def test_llm_connection():\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",  # Fast, free model for testing\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Reply only with 'API connected'\"}],\n",
    "            max_tokens=10,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        print(f\"‚úÖ LLM Connected: {response.choices[0].message.content}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM Failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_llm_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d06556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LLM integration code ready!\n"
     ]
    }
   ],
   "source": [
    "# CELL: Integrate with Your Generator\n",
    "def generate_with_llm(query, context):\n",
    "    \"\"\"Generate answer using Groq/Llama\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following context, answer the user's question.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=500,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"üöÄ LLM integration code ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85afdde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ REGULAR QUESTION PROMPT:\n",
      "Includes TRL section: False\n",
      "---\n",
      "üîπ TRL QUESTION PROMPT:\n",
      "Includes TRL section: True\n",
      "\n",
      "‚úÖ Universal prompt template ready!\n",
      "‚úÖ Automatically includes TRL guidance for maturity questions\n",
      "‚úÖ Single template for all query types\n"
     ]
    }
   ],
   "source": [
    "# CELL: Universal Prompt Template\n",
    "# Why: Single template that adapts to both regular and TRL queries automatically\n",
    "# What: Smart template that detects when to include maturity analysis\n",
    "\n",
    "UNIVERSAL_PROMPT_TEMPLATE = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER QUESTION:\n",
    "{question}\n",
    "\n",
    "ANALYSIS INSTRUCTIONS:\n",
    "1. Provide a comprehensive answer based strictly on the context provided\n",
    "2. Cite specific sources for each key point using [Source: filename]\n",
    "3. If the context is insufficient, acknowledge what cannot be answered\n",
    "\n",
    "{trl_section}\n",
    "\n",
    "ADDITIONAL GUIDELINES:\n",
    "- For technology maturity questions: assess development stage and transition evidence\n",
    "- For trend questions: identify velocity, drivers, and key players  \n",
    "- For forecasting: distinguish near-term vs long-term developments\n",
    "- For descriptive questions: provide specific examples and entities\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "def build_smart_prompt(question, context):\n",
    "    \"\"\"Build adaptive prompt that includes TRL guidance only when needed\"\"\"\n",
    "    \n",
    "    # Detect if this is a technology maturity question\n",
    "    maturity_keywords = ['trl', 'mature', 'transition', 'academy to application', \n",
    "                        'commercial', 'moving from academy', 'readiness', 'development stage']\n",
    "    \n",
    "    question_lower = question.lower()\n",
    "    is_maturity_question = any(keyword in question_lower for keyword in maturity_keywords)\n",
    "    \n",
    "    # Include TRL section only for maturity questions\n",
    "    if is_maturity_question:\n",
    "        trl_section = \"\"\"\n",
    "TECHNOLOGY MATURITY ASSESSMENT:\n",
    "- When discussing technology readiness, reference these stages:\n",
    "  * Research Phase (TRL 1-4): Basic research, lab validation\n",
    "  * Development Phase (TRL 5-6): Prototyping, testing  \n",
    "  * Commercialization Phase (TRL 7-9): Deployment, scaling\n",
    "- Assess current stage based on evidence in context\n",
    "- Identify transition indicators and timelines\n",
    "\"\"\"\n",
    "    else:\n",
    "        trl_section = \"\"\n",
    "    \n",
    "    prompt = UNIVERSAL_PROMPT_TEMPLATE.format(\n",
    "        context=context,\n",
    "        question=question,\n",
    "        trl_section=trl_section\n",
    "    )\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Test the universal template\n",
    "def test_universal_prompt():\n",
    "    \"\"\"Test that the template adapts to different question types\"\"\"\n",
    "    \n",
    "    test_context = \"Sample context about technology development...\"\n",
    "    \n",
    "    # Test regular question\n",
    "    regular_question = \"Which startups work on AI for automotive?\"\n",
    "    regular_prompt = build_smart_prompt(regular_question, test_context)\n",
    "    print(\"üîπ REGULAR QUESTION PROMPT:\")\n",
    "    print(\"Includes TRL section:\", \"TECHNOLOGY MATURITY ASSESSMENT\" in regular_prompt)\n",
    "    print(\"---\")\n",
    "    \n",
    "    # Test TRL question  \n",
    "    trl_question = \"Which quantum computing research is moving from academy to application?\"\n",
    "    trl_prompt = build_smart_prompt(trl_question, test_context)\n",
    "    print(\"üîπ TRL QUESTION PROMPT:\")\n",
    "    print(\"Includes TRL section:\", \"TECHNOLOGY MATURITY ASSESSMENT\" in trl_prompt)\n",
    "    \n",
    "    return regular_prompt, trl_prompt\n",
    "\n",
    "# Run test\n",
    "regular_prompt, trl_prompt = test_universal_prompt()\n",
    "\n",
    "print(\"\\n‚úÖ Universal prompt template ready!\")\n",
    "print(\"‚úÖ Automatically includes TRL guidance for maturity questions\")\n",
    "print(\"‚úÖ Single template for all query types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de134455",
   "metadata": {},
   "source": [
    "# Response Quality Setup\n",
    "\n",
    "**Why we're doing this:** \n",
    "Ensure answers are relevant and properly cite sources.\n",
    "\n",
    "**What we're doing:**\n",
    "\n",
    "- Checking if the pipeline works and our LLM integration and prompt template can return something nice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9e033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ TESTING COMPLETE RAG PIPELINE\n",
      "üß™ TESTING PIPELINE: 'Which startups work on AI for automotive?'\n",
      "==================================================\n",
      "1. üîç Retrieving documents...\n",
      "   ‚úÖ Found 3 relevant chunks\n",
      "2. üìù Building prompt...\n",
      "3. ü§ñ Generating answer with LLM...\n",
      "4. üìä RESULTS:\n",
      "QUESTION: Which startups work on AI for automotive?\n",
      "ANSWER: Based on the provided context, it is not possible to directly answer which startups work on AI for automotive. However, we can infer some information about the current state of AI in the automotive industry and potential future developments.\n",
      "\n",
      "The context suggests that generative AI technologies like GANs and VAEs have the potential to innovate and enhance various aspects of automotive design, manufacturing, and autonomous driving [Source: Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt]. Additionally, the development of domain-specific synthetic dialog datasets that incorporate disfluencies is crucial for enhancing the naturalness and adaptability of AI systems in real-world automotive environments [Source: drive_disfluency-rich_synthetic_dialog_data_generation_framework_for_intelligent_vehicle_environments.txt].\n",
      "\n",
      "Regarding AI-to-AI communication, recent advances in AI include models that can communicate with one another and create their own languages, which has implications for robotics, complex problem-solving, and other fields, including the automotive industry [Source: mckinsey_tech_trends_2025.txt].\n",
      "\n",
      "However, without specific information about startups working on AI for automotive, we cannot provide a comprehensive list of startups in this space. The provided context focuses on research papers and tech reports, which do not mention specific startups.\n",
      "\n",
      "To answer the question, we would need additional information about startups working on AI for automotive.\n",
      "\n",
      "üìö SOURCES:\n",
      "  1. Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt (Score: 0.475)\n",
      "  2. drive_disfluency-rich_synthetic_dialog_data_generation_framework_for_intelligent_vehicle_environments.txt (Score: 0.444)\n",
      "  3. mckinsey_tech_trends_2025.txt (Score: 0.426)\n",
      "\n",
      "üéâ PIPELINE SUCCESS!\n",
      "‚úÖ Question: Which startups work on AI for automotive?\n",
      "‚úÖ Answer generated: 1546 characters\n",
      "‚úÖ Sources used: 3 documents\n",
      "\n",
      "üìù Answer preview: Based on the provided context, it is not possible to directly answer which startups work on AI for automotive. However, we can infer some information about the current state of AI in the automotive in...\n"
     ]
    }
   ],
   "source": [
    "# CELL: Test Complete RAG Pipeline (CORRECTED)\n",
    "# Why: Use the actual dictionary structure from your retriever\n",
    "# What: Complete pipeline that works with your custom retriever output\n",
    "\n",
    "def test_complete_pipeline(question):\n",
    "    \"\"\"Test the full RAG pipeline\"\"\"\n",
    "    print(f\"üß™ TESTING PIPELINE: '{question}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Retrieve documents\n",
    "        print(\"1. üîç Retrieving documents...\")\n",
    "        retrieved_data = retriever.retrieve_with_sources(question, k=3)\n",
    "        print(f\"   ‚úÖ Found {len(retrieved_data)} relevant chunks\")\n",
    "        \n",
    "        # Step 2: Format context from the dictionaries\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Source: {item['source_file']} | Type: {item['doc_type']}\\nContent: {item['content']}\"\n",
    "            for item in retrieved_data\n",
    "        ])\n",
    "        \n",
    "        # Step 3: Build smart prompt\n",
    "        print(\"2. üìù Building prompt...\")\n",
    "        prompt = build_smart_prompt(question, context)\n",
    "        \n",
    "        # Step 4: Generate answer using LLM\n",
    "        print(\"3. ü§ñ Generating answer with LLM...\")\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=500,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Step 5: Display results\n",
    "        print(\"4. üìä RESULTS:\")\n",
    "        print(f\"QUESTION: {question}\")\n",
    "        print(f\"ANSWER: {answer}\")\n",
    "        print(\"\\nüìö SOURCES:\")\n",
    "        for i, item in enumerate(retrieved_data):\n",
    "            print(f\"  {i+1}. {item['source_file']} (Score: {item['similarity_score']:.3f})\")\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': retrieved_data,\n",
    "            'retrieved_chunks': len(retrieved_data)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pipeline error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Test the pipeline\n",
    "print(\"üöÄ TESTING COMPLETE RAG PIPELINE\")\n",
    "test_question = \"Which startups work on AI for automotive?\"\n",
    "result = test_complete_pipeline(test_question)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nüéâ PIPELINE SUCCESS!\")\n",
    "    print(f\"‚úÖ Question: {result['question']}\")\n",
    "    print(f\"‚úÖ Answer generated: {len(result['answer'])} characters\")\n",
    "    print(f\"‚úÖ Sources used: {len(result['sources'])} documents\")\n",
    "    \n",
    "    # Show a preview of the answer\n",
    "    print(f\"\\nüìù Answer preview: {result['answer'][:200]}...\")\n",
    "else:\n",
    "    print(\"\\nüí• Pipeline failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
