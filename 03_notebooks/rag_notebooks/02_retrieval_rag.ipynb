{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c14953",
   "metadata": {},
   "source": [
    "## Pre-processing and RAG retrieval testing\n",
    "\n",
    "#### What this script does:\n",
    "- Loads processed text files\n",
    "- Prepares documents for embedding by dividing into chunks \n",
    "- Creates embeddings using all-MiniLM-L6-v2\n",
    "- Saves vector index using FAISS in 04_models/vector_index/\n",
    "- Runs tests queries against index to check that retrieval returns relevant results\n",
    "- Prints the top-k documents and their similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a6fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from fastembed import TextEmbedding\n",
    "import faiss\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400ee47",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e09ac",
   "metadata": {},
   "source": [
    "1. Load reports and divide into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471bcca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading research papers...\n",
      "Loaded 61 chunks from enhanced_drift_aware_computer_vision_achitecture_for_autonomous_driving.txt\n",
      "Loaded 102 chunks from Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt\n",
      "Loaded 120 chunks from leveraging_vision_language_models_for_visual_grounding_and_analysis_of_automative_UI.txt\n",
      "Loaded 11408 chunks from automotive_papers_processed.txt\n",
      "Loaded 69 chunks from automating_automative_software_development_a_synergy_of_generative_AI_and_formal_methods.txt\n",
      "Loaded 137 chunks from automotive-software-and-electronics-2030-full-report.txt\n",
      "Loaded 102 chunks from AI_agents_in_engineering_design_a_multiagent_framework_for_aesthetic_and_aerodynamic_car_design.txt\n",
      "Loaded 87 chunks from a_benchmark_framework_for_AI_models_in_automative_aerodynamics.txt\n",
      "Loaded 227 chunks from generative_AI_for_autonomous_driving_a_review.txt\n",
      "Loaded 46 chunks from Embedded_acoustic_intelligence_for_automotive_systems.txt\n",
      "Loaded 107 chunks from drive_disfluency-rich_synthetic_dialog_data_generation_framework_for_intelligent_vehicle_environments.txt\n",
      "Loading patents data...\n",
      "Loaded 4198 chunks from automotive_patents_processed.txt\n",
      "Loading tech reports...\n",
      "Loaded 403 chunks from mckinsey_tech_trends_2025.txt\n",
      "Loaded 189 chunks from wef_emerging_tech_2025.txt\n",
      "Loaded 95 chunks from bcg_ai_value_2025.txt\n",
      "Loading startups data...\n",
      "Loaded 1350 chunks from autotechinsight_startups_processed.txt\n",
      "Loaded 16 chunks from seedtable_startups_processed.txt\n",
      "\n",
      "Summary:\n",
      "- Research papers: 12466 chunks\n",
      "- Patents data: 4198 chunks\n",
      "- Tech reports: 687 chunks\n",
      "- Startups data: 1366 chunks\n",
      "Total chunks created: 18717\n"
     ]
    }
   ],
   "source": [
    "# Define paths where processed text files are found \n",
    "data_path = \"../../01_data/rag_automotive_tech/processed\"\n",
    "papers_path = os.path.join(data_path, \"automotive_papers\") # Added journal abstracts file\n",
    "patents_path = os.path.join(data_path, \"automotive_tech_patents\") # Added patent file\n",
    "reports_path = os.path.join(data_path, \"tech_reports\")\n",
    "startups_path = os.path.join(data_path, \"startups\")  # Added startups files\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Function for chunking documents from folders\n",
    "def load_and_chunk_documents(folder_path, doc_type):\n",
    "    \"\"\"Load and chunk documents from a specific folder\"\"\"\n",
    "    chunks = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                loader = TextLoader(file_path, encoding='utf-8')\n",
    "                documents = loader.load()\n",
    "                \n",
    "                for doc in documents:\n",
    "                    doc.metadata.update({\n",
    "                        'source': filename,\n",
    "                        'doc_type': doc_type,\n",
    "                        'file_path': file_path\n",
    "                    })\n",
    "                \n",
    "                doc_chunks = text_splitter.split_documents(documents)\n",
    "                chunks.extend(doc_chunks)\n",
    "                print(f\"Loaded {len(doc_chunks)} chunks from {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Load and chunk all documents\n",
    "print(\"Loading research papers...\")\n",
    "papers_chunks = load_and_chunk_documents(papers_path, \"research_paper\")\n",
    "\n",
    "print(\"Loading patents data...\")\n",
    "patents_chunks = load_and_chunk_documents(patents_path, \"patents_data\")\n",
    "\n",
    "print(\"Loading tech reports...\")\n",
    "reports_chunks = load_and_chunk_documents(reports_path, \"tech_report\")\n",
    "\n",
    "print(\"Loading startups data...\")\n",
    "startups_chunks = load_and_chunk_documents(startups_path, \"startups\")\n",
    "\n",
    "# Combine all chunks\n",
    "all_chunks = papers_chunks + patents_chunks + reports_chunks + startups_chunks\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Research papers: {len(papers_chunks)} chunks\")\n",
    "print(f\"- Patents data: {len(patents_chunks)} chunks\")\n",
    "print(f\"- Tech reports: {len(reports_chunks)} chunks\")\n",
    "print(f\"- Startups data: {len(startups_chunks)} chunks\")\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3eed3",
   "metadata": {},
   "source": [
    "2. Create embeddings and save FAISS Vector Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe142c9",
   "metadata": {},
   "source": [
    "\n",
    "Vectors were generated using the model sentence-transformers/all-MiniLM-L6-v2, and then stored in a FAISS index.\n",
    "\n",
    "sentence-transformers/all-MiniLM-L6-v2 is a widely used embedding model that was designed for semantic similarity, sentence clustering and small to medium-scale retrieval\n",
    "\n",
    "FAISS is a vector search engine for storing and searching embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44968c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ RELIABLE FAISS - Processing 18717 chunks\n",
      "Loading fast embedding model...\n",
      "Creating embeddings for 18717 chunks...\n",
      "‚úì Created 18717 embeddings\n",
      "Embedding shape: (18717, 384)\n",
      "\n",
      "üîß Creating FAISS index...\n",
      "‚úì FAISS index created with 18717 vectors\n",
      "\n",
      "üíæ Saving to disk...\n",
      "‚úì FAISS index saved: ../../04_models/vector_index/faiss_index.bin\n",
      "‚úì Texts saved: ../../04_models/vector_index/texts.pkl (18717 texts)\n",
      "‚úì Metadata saved: ../../04_models/vector_index/metadata.pkl (18717 entries)\n",
      "‚úì Embeddings saved: ../../04_models/vector_index/embeddings.npy\n",
      "‚úì Index info saved: ../../04_models/vector_index/index_info.json\n",
      "\n",
      "==================================================\n",
      "‚úÖ FAISS INDEX CREATION COMPLETE\n",
      "==================================================\n",
      "\n",
      "üìä STATS:\n",
      "Total chunks: 18717\n",
      "Embedding dimension: 384\n",
      "FAISS index size: 18717 vectors\n",
      "\n",
      "üß™ TEST QUERY:\n",
      "Query: 'automotive technology'\n",
      "Found 3 results:\n",
      "\n",
      "  Result 1 (distance: 0.6645):\n",
      "    Type: research_paper\n",
      "    Preview: (V2X), Internet of Things (IOT), public clouds, data analytics, artificial intelligence, digitalizat...\n",
      "\n",
      "  Result 2 (distance: 0.7099):\n",
      "    Type: research_paper\n",
      "    Preview: RESEARCH PAPER #13:\n",
      "  Title: Emerging Trends in the Automotive Industry Driven by Sustainable Techno...\n",
      "\n",
      "  Result 3 (distance: 0.7124):\n",
      "    Type: research_paper\n",
      "    Preview: RESEARCH PAPER #919:\n",
      "  Title: Mapping the Landscape of Romanian Automotive Research: A Bibliometric ...\n",
      "\n",
      "üìÅ Directory contents of ../../04_models/vector_index:\n",
      "  - chunks_metadata.pkl (11,772,358 bytes)\n",
      "  - metadata.pkl (431,988 bytes)\n",
      "  - faiss_index.bin (28,749,357 bytes)\n",
      "  - embeddings.npy (28,749,440 bytes)\n",
      "  - index_info.json (187 bytes)\n",
      "  - texts.pkl (11,096,569 bytes)\n",
      "\n",
      "==================================================\n",
      "üéâ RELIABLE VECTOR STORE READY!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and save using FAISS\n",
    "\n",
    "print(f\"Processing {len(all_chunks)} chunks\")\n",
    "\n",
    "# Extract texts\n",
    "texts = [chunk.page_content for chunk in all_chunks]\n",
    "metadatas = [chunk.metadata for chunk in all_chunks]\n",
    "\n",
    "# Setup paths\n",
    "vector_index_path = \"../../04_models/vector_index\"\n",
    "os.makedirs(vector_index_path, exist_ok=True)\n",
    "\n",
    "# 1. Create embeddings\n",
    "print(\"Loading fast embedding model...\")\n",
    "model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"Creating embeddings for {len(texts)} chunks...\")\n",
    "embeddings = list(model.embed(texts))\n",
    "print(f\"‚úì Created {len(embeddings)} embeddings\")\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings_array = np.array([emb.tolist() for emb in embeddings]).astype('float32')\n",
    "print(f\"Embedding shape: {embeddings_array.shape}\")\n",
    "\n",
    "# 2. Create FAISS index\n",
    "print(\"\\nüîß Creating FAISS index...\")\n",
    "dimension = embeddings_array.shape[1]\n",
    "\n",
    "# Create index (L2 distance - smaller is better)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "print(f\"‚úì FAISS index created with {index.ntotal} vectors\")\n",
    "\n",
    "# 3. Save everything\n",
    "print(\"\\nüíæ Saving to disk...\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss_index_path = os.path.join(vector_index_path, \"faiss_index.bin\")\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "print(f\"‚úì FAISS index saved: {faiss_index_path}\")\n",
    "\n",
    "# Save texts\n",
    "texts_path = os.path.join(vector_index_path, \"texts.pkl\")\n",
    "with open(texts_path, \"wb\") as f:\n",
    "    pickle.dump(texts, f)\n",
    "print(f\"‚úì Texts saved: {texts_path} ({len(texts)} texts)\")\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = os.path.join(vector_index_path, \"metadata.pkl\")\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(metadatas, f)\n",
    "print(f\"‚úì Metadata saved: {metadata_path} ({len(metadatas)} entries)\")\n",
    "\n",
    "# Save embeddings for reference\n",
    "embeddings_path = os.path.join(vector_index_path, \"embeddings.npy\")\n",
    "np.save(embeddings_path, embeddings_array)\n",
    "print(f\"‚úì Embeddings saved: {embeddings_path}\")\n",
    "\n",
    "# 4. Create index info\n",
    "index_info = {\n",
    "    \"total_chunks\": len(texts),\n",
    "    \"embedding_dim\": dimension,\n",
    "    \"created_at\": str(datetime.now()),\n",
    "    \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"index_type\": \"FAISS IndexFlatL2\"\n",
    "}\n",
    "\n",
    "info_path = os.path.join(vector_index_path, \"index_info.json\")\n",
    "import json\n",
    "with open(info_path, \"w\") as f:\n",
    "    json.dump(index_info, f, indent=2)\n",
    "print(f\"‚úì Index info saved: {info_path}\")\n",
    "\n",
    "# 5. VERIFICATION\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ FAISS INDEX CREATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nüìä STATS:\")\n",
    "print(f\"Total chunks: {len(texts)}\")\n",
    "print(f\"Embedding dimension: {dimension}\")\n",
    "print(f\"FAISS index size: {index.ntotal} vectors\")\n",
    "\n",
    "# Test query\n",
    "print(\"\\nüß™ TEST QUERY:\")\n",
    "test_query = \"automotive technology\"\n",
    "print(f\"Query: '{test_query}'\")\n",
    "\n",
    "# Create embedding for query\n",
    "query_embedding = np.array(list(model.embed([test_query]))[0].tolist()).astype('float32').reshape(1, -1)\n",
    "\n",
    "# Search\n",
    "k = 3\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "\n",
    "print(f\"Found {len(indices[0])} results:\")\n",
    "for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "    if idx != -1:  # FAISS returns -1 if not enough results\n",
    "        preview = texts[idx][:100] + \"...\" if len(texts[idx]) > 100 else texts[idx]\n",
    "        doc_type = metadatas[idx].get('doc_type', 'unknown')\n",
    "        print(f\"\\n  Result {i+1} (distance: {distance:.4f}):\")\n",
    "        print(f\"    Type: {doc_type}\")\n",
    "        print(f\"    Preview: {preview}\")\n",
    "\n",
    "# 6. Directory listing\n",
    "print(f\"\\nüìÅ Directory contents of {vector_index_path}:\")\n",
    "for file in os.listdir(vector_index_path):\n",
    "    file_path = os.path.join(vector_index_path, file)\n",
    "    size = os.path.getsize(file_path)\n",
    "    print(f\"  - {file} ({size:,} bytes)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ RELIABLE VECTOR STORE READY!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a270b5",
   "metadata": {},
   "source": [
    "### Retrieval test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9378ade",
   "metadata": {},
   "source": [
    "Load and test the FAISS vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9c42bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Direct FAISS index test from Notebook 2\n",
      "============================================================\n",
      "Index path: ../../04_models/vector_index\n",
      "Path exists: True\n",
      "\n",
      "üìÅ Directory contents:\n",
      "  - metadata.pkl (431,988 bytes)\n",
      "  - faiss_index.bin (28,749,357 bytes)\n",
      "  - embeddings.npy (28,749,440 bytes)\n",
      "  - index_info.json (187 bytes)\n",
      "  - texts.pkl (11,096,569 bytes)\n",
      "\n",
      "‚úÖ FAISS index loaded: 18717 vectors\n",
      "‚úÖ Texts loaded: 18717 chunks\n",
      "‚úÖ Metadata loaded: 18717 entries\n",
      "‚úÖ Embedding model loaded\n",
      "\n",
      "============================================================\n",
      "üß™ Testing FAISS search directly:\n",
      "\n",
      "üîç Query: 'automotive startups'\n",
      "  Result 1:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.5672\n",
      "  Result 2:\n",
      "    Type: startups\n",
      "    Source: autotechinsight_startups_processed.txt\n",
      "    Similarity: 0.5527\n",
      "    üöÄ Startups data: ‚úì\n",
      "  Result 3:\n",
      "    Type: startups\n",
      "    Source: autotechinsight_startups_processed.txt\n",
      "    Similarity: 0.5459\n",
      "    üöÄ Startups data: ‚úì\n",
      "\n",
      "üîç Query: 'autonomous driving technology'\n",
      "  Result 1:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.6073\n",
      "  Result 2:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.5911\n",
      "  Result 3:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.5786\n",
      "\n",
      "üîç Query: 'generative AI in automotive'\n",
      "  Result 1:\n",
      "    Type: research_paper\n",
      "    Source: Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt\n",
      "    Similarity: 0.7408\n",
      "  Result 2:\n",
      "    Type: research_paper\n",
      "    Source: Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt\n",
      "    Similarity: 0.7183\n",
      "  Result 3:\n",
      "    Type: research_paper\n",
      "    Source: Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt\n",
      "    Similarity: 0.7065\n",
      "\n",
      "üîç Query: 'electric vehicle innovation'\n",
      "  Result 1:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.6837\n",
      "  Result 2:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.6667\n",
      "  Result 3:\n",
      "    Type: research_paper\n",
      "    Source: automotive_papers_processed.txt\n",
      "    Similarity: 0.6167\n",
      "\n",
      "============================================================\n",
      "üìà Document type distribution:\n",
      "  - research_paper: 12466 chunks\n",
      "  - patents_data: 4198 chunks\n",
      "  - startups: 1366 chunks\n",
      "  - tech_report: 687 chunks\n",
      "\n",
      "============================================================\n",
      "‚úÖ Direct FAISS test completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Path to your FAISS index\n",
    "VECTOR_INDEX_PATH = \"../../04_models/vector_index\"\n",
    "\n",
    "print(f\"Index path: {VECTOR_INDEX_PATH}\")\n",
    "print(f\"Path exists: {os.path.exists(VECTOR_INDEX_PATH)}\")\n",
    "\n",
    "if os.path.exists(VECTOR_INDEX_PATH):\n",
    "    print(f\"\\nüìÅ Directory contents:\")\n",
    "    for file in os.listdir(VECTOR_INDEX_PATH):\n",
    "        file_path = os.path.join(VECTOR_INDEX_PATH, file)\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"  - {file} ({size:,} bytes)\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load FAISS index\n",
    "        index_path = os.path.join(VECTOR_INDEX_PATH, \"faiss_index.bin\")\n",
    "        index = faiss.read_index(index_path)\n",
    "        print(f\"\\n‚úÖ FAISS index loaded: {index.ntotal} vectors\")\n",
    "        \n",
    "        # 2. Load texts\n",
    "        texts_path = os.path.join(VECTOR_INDEX_PATH, \"texts.pkl\")\n",
    "        with open(texts_path, \"rb\") as f:\n",
    "            texts = pickle.load(f)\n",
    "        print(f\"‚úÖ Texts loaded: {len(texts)} chunks\")\n",
    "        \n",
    "        # 3. Load metadata\n",
    "        metadata_path = os.path.join(VECTOR_INDEX_PATH, \"metadata.pkl\")\n",
    "        with open(metadata_path, \"rb\") as f:\n",
    "            metadatas = pickle.load(f)\n",
    "        print(f\"‚úÖ Metadata loaded: {len(metadatas)} entries\")\n",
    "        \n",
    "        # 4. Initialize embedding model for queries\n",
    "        model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        print(\"‚úÖ Embedding model loaded\")\n",
    "        \n",
    "        # 5. Test queries\n",
    "        test_queries = [\n",
    "            \"automotive startups\",\n",
    "            \"autonomous driving technology\", \n",
    "            \"generative AI in automotive\",\n",
    "            \"electric vehicle innovation\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üß™ Testing FAISS search directly:\")\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nüîç Query: '{query}'\")\n",
    "            \n",
    "            # Create query embedding\n",
    "            query_embedding = np.array(\n",
    "                list(model.embed([query]))[0].tolist()\n",
    "            ).astype('float32').reshape(1, -1)\n",
    "            \n",
    "            # Search\n",
    "            k = 3\n",
    "            distances, indices = index.search(query_embedding, k)\n",
    "            \n",
    "            # Show results\n",
    "            found_results = 0\n",
    "            for i, (idx, distance) in enumerate(zip(indices[0], distances[0])):\n",
    "                if idx != -1 and idx < len(texts):\n",
    "                    similarity = 1.0 / (1.0 + distance)\n",
    "                    if similarity > 0.5:  # Threshold\n",
    "                        found_results += 1\n",
    "                        doc_type = metadatas[idx].get('doc_type', 'N/A') if idx < len(metadatas) else 'N/A'\n",
    "                        source = metadatas[idx].get('source', 'N/A') if idx < len(metadatas) else 'N/A'\n",
    "                        \n",
    "                        print(f\"  Result {i+1}:\")\n",
    "                        print(f\"    Type: {doc_type}\")\n",
    "                        print(f\"    Source: {source}\")\n",
    "                        print(f\"    Similarity: {similarity:.4f}\")\n",
    "                        \n",
    "                        # Check if startups\n",
    "                        if doc_type == 'startups':\n",
    "                            print(f\"    üöÄ Startups data: ‚úì\")\n",
    "            \n",
    "            if found_results == 0:\n",
    "                print(f\"  ‚ùå No results above threshold 0.5\")\n",
    "        \n",
    "        # Show document distribution\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üìà Document type distribution:\")\n",
    "        doc_type_counts = {}\n",
    "        for metadata in metadatas:\n",
    "            doc_type = metadata.get('doc_type', 'unknown')\n",
    "            doc_type_counts[doc_type] = doc_type_counts.get(doc_type, 0) + 1\n",
    "        \n",
    "        for doc_type, count in sorted(doc_type_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  - {doc_type}: {count} chunks\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"‚úÖ Direct FAISS test completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during direct test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ùå Vector index directory not found!\")\n",
    "    print(\"Make sure you ran the FAISS embedding creation code first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
