{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5c14953",
   "metadata": {},
   "source": [
    "## Pre-processing and building RAG Retrieval Prototype\n",
    "\n",
    "#### What this script does:\n",
    "- Loads processed text files\n",
    "- Prepares documents for embedding by dividing into chunks \n",
    "- Creates embeddings using TF-IDF\n",
    "- Saves vector index in 04_models/vector_index/\n",
    "- Loads TF-IDF retriever from rag_componets/retriever.py\n",
    "- Tests retriever with some basic automotive-focused questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611a6fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b400ee47",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e09ac",
   "metadata": {},
   "source": [
    "1. Load reports and divide into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471bcca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading research papers...\n",
      "Loaded 61 chunks from enhanced_drift_aware_computer_vision_achitecture_for_autonomous_driving.txt\n",
      "Loaded 102 chunks from Gen_AI_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt\n",
      "Loaded 120 chunks from leveraging_vision_language_models_for_visual_grounding_and_analysis_of_automative_UI.txt\n",
      "Loaded 11408 chunks from automotive_papers_processed.txt\n",
      "Loaded 69 chunks from automating_automative_software_development_a_synergy_of_generative_AI_and_formal_methods.txt\n",
      "Loaded 137 chunks from automotive-software-and-electronics-2030-full-report.txt\n",
      "Loaded 102 chunks from AI_agents_in_engineering_design_a_multiagent_framework_for_aesthetic_and_aerodynamic_car_design.txt\n",
      "Loaded 87 chunks from a_benchmark_framework_for_AI_models_in_automative_aerodynamics.txt\n",
      "Loaded 227 chunks from generative_AI_for_autonomous_driving_a_review.txt\n",
      "Loaded 46 chunks from Embedded_acoustic_intelligence_for_automotive_systems.txt\n",
      "Loaded 107 chunks from drive_disfluency-rich_synthetic_dialog_data_generation_framework_for_intelligent_vehicle_environments.txt\n",
      "Loading patents data...\n",
      "Loaded 4198 chunks from automotive_patents_processed.txt\n",
      "Loading tech reports...\n",
      "Loaded 403 chunks from mckinsey_tech_trends_2025.txt\n",
      "Loaded 189 chunks from wef_emerging_tech_2025.txt\n",
      "Loaded 95 chunks from bcg_ai_value_2025.txt\n",
      "Loading startups data...\n",
      "Loaded 17293 chunks from startups_processed.txt\n",
      "\n",
      "Summary:\n",
      "- Research papers: 12466 chunks\n",
      "- Patents data: 4198 chunks\n",
      "- Tech reports: 687 chunks\n",
      "- Startups data: 17293 chunks\n",
      "Total chunks created: 34644\n"
     ]
    }
   ],
   "source": [
    "# Define paths where processed text files are found \n",
    "data_path = \"../../01_data/rag_automotive_tech/processed\"\n",
    "papers_path = os.path.join(data_path, \"automotive_papers\") # Added journal abstracts file\n",
    "patents_path = os.path.join(data_path, \"automotive_tech_patents\") # Added patent file\n",
    "reports_path = os.path.join(data_path, \"tech_reports\")\n",
    "startups_file = os.path.join(data_path, \"startups_processed.txt\")  # Added startups file\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Function for chunking documents from folders\n",
    "def load_and_chunk_documents(folder_path, doc_type):\n",
    "    \"\"\"Load and chunk documents from a specific folder\"\"\"\n",
    "    chunks = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                loader = TextLoader(file_path, encoding='utf-8')\n",
    "                documents = loader.load()\n",
    "                \n",
    "                for doc in documents:\n",
    "                    doc.metadata.update({\n",
    "                        'source': filename,\n",
    "                        'doc_type': doc_type,\n",
    "                        'file_path': file_path\n",
    "                    })\n",
    "                \n",
    "                doc_chunks = text_splitter.split_documents(documents)\n",
    "                chunks.extend(doc_chunks)\n",
    "                print(f\"Loaded {len(doc_chunks)} chunks from {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Function for chunking single files\n",
    "def load_and_chunk_single_file(file_path, doc_type, source_name):\n",
    "    \"\"\"Load and chunk a single file\"\"\"\n",
    "    chunks = []\n",
    "    try:\n",
    "        loader = TextLoader(file_path, encoding='utf-8')\n",
    "        documents = loader.load()\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                'source': source_name,\n",
    "                'doc_type': doc_type,\n",
    "                'file_path': file_path\n",
    "            })\n",
    "        \n",
    "        doc_chunks = text_splitter.split_documents(documents)\n",
    "        chunks.extend(doc_chunks)\n",
    "        print(f\"Loaded {len(doc_chunks)} chunks from {source_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {source_name}: {e}\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Load and chunk all documents\n",
    "print(\"Loading research papers...\")\n",
    "papers_chunks = load_and_chunk_documents(papers_path, \"research_paper\")\n",
    "\n",
    "print(\"Loading patents data...\")\n",
    "patents_chunks = load_and_chunk_documents(patents_path, \"patents_data\")\n",
    "\n",
    "print(\"Loading tech reports...\")\n",
    "reports_chunks = load_and_chunk_documents(reports_path, \"tech_report\")\n",
    "\n",
    "print(\"Loading startups data...\")\n",
    "startups_chunks = load_and_chunk_single_file(startups_file, \"startups_data\", \"startups_processed.txt\")\n",
    "\n",
    "# Combine all chunks\n",
    "all_chunks = papers_chunks + patents_chunks + reports_chunks + startups_chunks\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"- Research papers: {len(papers_chunks)} chunks\")\n",
    "print(f\"- Patents data: {len(patents_chunks)} chunks\")\n",
    "print(f\"- Tech reports: {len(reports_chunks)} chunks\")\n",
    "print(f\"- Startups data: {len(startups_chunks)} chunks\")\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3eed3",
   "metadata": {},
   "source": [
    "2. Create embeddings using TF-IDF and save Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d83c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks to embed: 34644\n",
      "Creating TF-IDF embeddings...\n",
      "âœ“ TF-IDF embeddings created and saved!\n",
      "Saving chunks metadata...\n",
      "âœ“ Chunks metadata saved successfully!\n",
      "âœ“ Embedding process completed! Files saved in: ../../04_models/vector_index\n",
      "\n",
      "ğŸ“ Files in vector_index directory:\n",
      "  - chunks_metadata.pkl\n",
      "  - tfidf_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Confirm that we have all chunks\n",
    "print(f\"Total chunks to embed: {len(all_chunks)}\")\n",
    "\n",
    "# Create directory for vector storage\n",
    "vector_index_path = \"../../04_models/vector_index\"\n",
    "os.makedirs(vector_index_path, exist_ok=True)\n",
    "\n",
    "print(\"Creating TF-IDF embeddings...\")\n",
    "\n",
    "# Extract text from chunks\n",
    "texts = [chunk.page_content for chunk in all_chunks]\n",
    "\n",
    "# Build TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Package TF-IDF data\n",
    "tfidf_data = {\n",
    "    \"matrix\": tfidf_matrix,\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"chunks\": [\n",
    "        {\n",
    "            \"page_content\": chunk.page_content,\n",
    "            \"metadata\": chunk.metadata\n",
    "        }\n",
    "        for chunk in all_chunks\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save TF-IDF model + matrix\n",
    "joblib.dump(tfidf_data, os.path.join(vector_index_path, \"tfidf_embeddings.pkl\"))\n",
    "print(\"âœ“ TF-IDF embeddings created and saved!\")\n",
    "\n",
    "# Save chunk metadata (optional but useful)\n",
    "print(\"Saving chunks metadata...\")\n",
    "chunks_metadata = [\n",
    "    {\n",
    "        \"page_content\": chunk.page_content,\n",
    "        \"metadata\": chunk.metadata,\n",
    "        \"embedding_index\": i\n",
    "    }\n",
    "    for i, chunk in enumerate(all_chunks)\n",
    "]\n",
    "\n",
    "with open(os.path.join(vector_index_path, \"chunks_metadata.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(chunks_metadata, f)\n",
    "\n",
    "print(\"âœ“ Chunks metadata saved successfully!\")\n",
    "print(f\"âœ“ Embedding process completed! Files saved in: {vector_index_path}\")\n",
    "\n",
    "# List created files\n",
    "print(\"\\nğŸ“ Files in vector_index directory:\")\n",
    "for file in os.listdir(vector_index_path):\n",
    "    print(f\"  - {file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a270b5",
   "metadata": {},
   "source": [
    "### Retriever prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9378ade",
   "metadata": {},
   "source": [
    "Load and test the retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c79f7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Retriever imported!\n"
     ]
    }
   ],
   "source": [
    "def import_retriever():\n",
    "    current_dir = os.getcwd()\n",
    "    retriever_path = os.path.join(current_dir, 'rag_components', 'retriever.py')\n",
    "    spec = importlib.util.spec_from_file_location(\"retriever\", retriever_path)\n",
    "    retriever_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(retriever_module)\n",
    "    return retriever_module.DocumentAwareRetriever\n",
    "\n",
    "DocumentAwareRetriever = import_retriever()\n",
    "print(\"âœ… Retriever imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0365480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TF-IDF retriever...\n",
      "âœ“ TF-IDF retriever loaded successfully\n",
      "\n",
      "ğŸ§ª Testing retriever with automotive-focused queries...\n",
      "\n",
      "ğŸ” Query: 'automotive startups'\n",
      "ğŸ“Š Top result: startups_processed.txt\n",
      "ğŸ“ Type: startups_data\n",
      "â­ Score: 0.5538\n",
      "ğŸš€ Startups data included: âœ“\n",
      "\n",
      "ğŸ” Query: 'autonomous driving technology'\n",
      "ğŸ“Š Top result: automotive_papers_processed.txt\n",
      "ğŸ“ Type: research_paper\n",
      "â­ Score: 0.6976\n",
      "ğŸš€ Startups data included: âœ—\n",
      "\n",
      "ğŸ” Query: 'generative AI in automotive'\n",
      "ğŸ“Š Top result: automotive_patents_processed.txt\n",
      "ğŸ“ Type: patents_data\n",
      "â­ Score: 0.8146\n",
      "ğŸš€ Startups data included: âœ—\n",
      "\n",
      "ğŸ” Query: 'electric vehicle innovation'\n",
      "ğŸ“Š Top result: automotive_patents_processed.txt\n",
      "ğŸ“ Type: patents_data\n",
      "â­ Score: 0.5223\n",
      "ğŸš€ Startups data included: âœ—\n",
      "\n",
      "ğŸ“ˆ Document type distribution:\n",
      "  - research_paper: 12466 chunks\n",
      "  - patents_data: 4198 chunks\n",
      "  - tech_report: 687 chunks\n",
      "  - startups_data: 17293 chunks\n"
     ]
    }
   ],
   "source": [
    "# Initialize the retriever\n",
    "print(\"Initializing TF-IDF retriever...\")\n",
    "VECTOR_INDEX_PATH = \"../../04_models/vector_index\"\n",
    "retriever = DocumentAwareRetriever(VECTOR_INDEX_PATH)\n",
    "\n",
    "# Test the retriever with startups data\n",
    "print(\"\\nğŸ§ª Testing retriever with automotive-focused queries...\")\n",
    "\n",
    "test_queries = [\n",
    "    \"automotive startups\",\n",
    "    \"autonomous driving technology\", \n",
    "    \"generative AI in automotive\",\n",
    "    \"electric vehicle innovation\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = retriever.retrieve_with_sources(query, k=3)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"\\nğŸ” Query: '{query}'\")\n",
    "        print(f\"ğŸ“Š Top result: {results[0]['source_file']}\")\n",
    "        print(f\"ğŸ“ Type: {results[0]['doc_type']}\")\n",
    "        print(f\"â­ Score: {results[0]['similarity_score']:.4f}\")\n",
    "        \n",
    "        # Check if startups data was retrieved\n",
    "        startups_found = any(doc['doc_type'] == 'startups_data' for doc in results)\n",
    "        print(f\"ğŸš€ Startups data included: {'âœ“' if startups_found else 'âœ—'}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ No results for: '{query}'\")\n",
    "\n",
    "# Show document type distribution\n",
    "doc_type_counts = retriever.get_doc_type_counts()\n",
    "print(f\"\\nğŸ“ˆ Document type distribution:\")\n",
    "for doc_type, count in doc_type_counts.items():\n",
    "    print(f\"  - {doc_type}: {count} chunks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
