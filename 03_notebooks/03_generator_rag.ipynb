{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93446b4",
   "metadata": {},
   "source": [
    "# LLM Response Generation\n",
    "\n",
    "**Why we're doing this:**\n",
    " Take retrieved document chunks and generate coherent answers using a language model.\n",
    "\n",
    "**What we're doing:**\n",
    "\n",
    "- Setting up first prototype - done\n",
    "- Setting up the LLM client (Groq/Llama) - done\n",
    "- Creating prompt templates for TRL questions - done\n",
    "- Generating answers from retrieved context - done "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7a9148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ COMPONENTS IMPORTED SUCCESSFULLY!\n",
      "‚úì TF-IDF retriever loaded successfully\n",
      "‚úì Template-based RAG answer generator initialized\n",
      "‚úÖ Generation pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "# PERMANENT WORKING IMPORT - USE THIS EVERYWHERE\n",
    "import sys\n",
    "import os\n",
    "import importlib.util\n",
    "\n",
    "def import_rag_components():\n",
    "    \"\"\"Import RAG components using absolute paths - guaranteed to work\"\"\"\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # Import retriever\n",
    "    retriever_path = os.path.join(current_dir, 'rag_components', 'retriever.py')\n",
    "    spec = importlib.util.spec_from_file_location(\"retriever\", retriever_path)\n",
    "    retriever_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(retriever_module)\n",
    "    \n",
    "    # Import query_interface  \n",
    "    query_interface_path = os.path.join(current_dir, 'rag_components', 'query_interface.py')\n",
    "    spec = importlib.util.spec_from_file_location(\"query_interface\", query_interface_path)\n",
    "    query_interface_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(query_interface_module)\n",
    "    \n",
    "    # Import answer_generator\n",
    "    answer_generator_path = os.path.join(current_dir, 'rag_components', 'answer_generator.py')\n",
    "    spec = importlib.util.spec_from_file_location(\"answer_generator\", answer_generator_path)\n",
    "    answer_generator_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(answer_generator_module)\n",
    "    \n",
    "    return (retriever_module.DocumentAwareRetriever, \n",
    "            query_interface_module.SimpleQueryInterface,\n",
    "            answer_generator_module.RAGAnswerGenerator)\n",
    "\n",
    "# Import the components\n",
    "DocumentAwareRetriever, SimpleQueryInterface, RAGAnswerGenerator = import_rag_components()\n",
    "print(\"üéâ COMPONENTS IMPORTED SUCCESSFULLY!\")\n",
    "\n",
    "# Continue with code\n",
    "VECTOR_INDEX_PATH = \"../04_models/vector_index\"\n",
    "retriever = DocumentAwareRetriever(VECTOR_INDEX_PATH)\n",
    "query_interface = SimpleQueryInterface(retriever)\n",
    "answer_generator = RAGAnswerGenerator(query_interface)\n",
    "print(\"‚úÖ Generation pipeline ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b256e76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting groq\n",
      "  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from groq) (4.11.0)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from groq) (2.12.4)\n",
      "Requirement already satisfied: sniffio in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->groq) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->groq) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/manueltimowolf/Library/Python/3.9/lib/python/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
      "Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: distro, groq\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [groq][32m1/2\u001b[0m [groq]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 groq-0.36.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14b686a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Groq client initialized successfully\n",
      "üéâ LLM client ready for integration!\n"
     ]
    }
   ],
   "source": [
    "# CELL: LLM Client Setup\n",
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client\n",
    "def setup_groq_client():\n",
    "    \"\"\"Set up and return Groq client with error handling\"\"\"\n",
    "    api_key = os.getenv('GROQ_API_KEY')\n",
    "    \n",
    "    if not api_key:\n",
    "        raise ValueError(\"‚ùå GROQ_API_KEY not found in environment variables\")\n",
    "    \n",
    "    client = Groq(api_key=api_key)\n",
    "    print(\"‚úÖ Groq client initialized successfully\")\n",
    "    return client\n",
    "\n",
    "# Test the client\n",
    "try:\n",
    "    groq_client = setup_groq_client()\n",
    "    print(\"üéâ LLM client ready for integration!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize LLM client: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "962956a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Connected: API connected\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CELL: Test LLM Connection\n",
    "# Why: Verify Groq API works and model responds correctly\n",
    "# What: Send simple test query to confirm setup is functional\n",
    "def test_llm_connection():\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",  # Fast, free model for testing\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Reply only with 'API connected'\"}],\n",
    "            max_tokens=10,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        print(f\"‚úÖ LLM Connected: {response.choices[0].message.content}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM Failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_llm_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95d06556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ LLM integration code ready!\n"
     ]
    }
   ],
   "source": [
    "# CELL: Integrate with Your Generator\n",
    "def generate_with_llm(query, context):\n",
    "    \"\"\"Generate answer using Groq/Llama\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on the following context, answer the user's question.\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama3-8b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=500,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"üöÄ LLM integration code ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72ab688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TRL-specific prompt template added for maturity analysis\n"
     ]
    }
   ],
   "source": [
    "# CELL: TRL-Specific Prompt Template\n",
    "# Why: Provide structured guidance for technology maturity assessment\n",
    "# What: Template that incorporates TRL definitions for transition analysis\n",
    "\n",
    "TRL_TRANSITION_TEMPLATE = \"\"\"\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "TRL DEFINITIONS:\n",
    "- TRL 1-2: Basic research, scientific formulation\n",
    "- TRL 3-4: Experimental proof of concept, lab validation  \n",
    "- TRL 5-6: Technology demonstration, prototype testing\n",
    "- TRL 7-8: System prototype, operational environment\n",
    "- TRL 9: Actual system proven, commercial deployment\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "ANALYSIS REQUEST:\n",
    "Based on the context and TRL definitions above, analyze which research topics are transitioning from academic research (TRL 1-4) to commercial application (TRL 5-9). For each identified technology:\n",
    "\n",
    "1. Current estimated TRL level\n",
    "2. Evidence of transition/commercialization\n",
    "3. Key players or organizations involved\n",
    "4. Timeline indicators for maturity\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATES = {\n",
    "    \"technical\": \"\"\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer based on the context. If unsure, say so. Cite sources.\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    \"trl_transition\": TRL_TRANSITION_TEMPLATE,\n",
    "    \n",
    "    \"trends\": \"\"\"\n",
    "Context: {context}\n",
    "What are the trends in: {question}\n",
    "Identify 3-5 key trends with evidence:\n",
    "Answer:\"\"\"\n",
    "}\n",
    "\n",
    "def get_prompt_template(question):\n",
    "    \"\"\"Select appropriate template based on question content\"\"\"\n",
    "    if any(keyword in question.lower() for keyword in ['trl', 'mature', 'transition', 'academy to application', 'commercial']):\n",
    "        return PROMPT_TEMPLATES[\"trl_transition\"]\n",
    "    elif any(keyword in question.lower() for keyword in ['trend', 'growing', 'latest']):\n",
    "        return PROMPT_TEMPLATES[\"trends\"]\n",
    "    else:\n",
    "        return PROMPT_TEMPLATES[\"technical\"]\n",
    "\n",
    "print(\"‚úÖ TRL-specific prompt template added for maturity analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d0ecb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optimized prompt templates - TRL only for relevant queries\n",
      "‚úÖ Queries 1-7 use clean templates, 8-9 get TRL context when needed\n"
     ]
    }
   ],
   "source": [
    "# CELL: Optimized Prompt Templates\n",
    "# Why: Provide specialized prompts only when TRL context is relevant\n",
    "# What: Keep templates clean and focused on actual query needs\n",
    "\n",
    "PROMPT_TEMPLATES = {\n",
    "    \"descriptive\": \"\"\"\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Provide a comprehensive overview based on the context\n",
    "- List specific examples and entities mentioned\n",
    "- Cite sources for each key point\n",
    "- If context is insufficient, acknowledge limitations\n",
    "\n",
    "ANSWER:\"\"\",\n",
    "    \n",
    "    \"explanatory\": \"\"\"\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Provide detailed explanations with examples\n",
    "- Break down complex concepts into understandable parts\n",
    "- Show relationships between different elements\n",
    "- Cite specific sources that support your explanation\n",
    "\n",
    "ANSWER:\"\"\",\n",
    "    \n",
    "    \"trends\": \"\"\"\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Identify 3-5 key trends with supporting evidence\n",
    "- Note the velocity and direction of each trend\n",
    "- Mention any disruptive or accelerating factors\n",
    "- Provide timeline context where available\n",
    "- Cite sources for each trend identified\n",
    "\n",
    "ANSWER:\"\"\",\n",
    "    \n",
    "    \"forecast\": \"\"\"\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Analyze timing and maturity indicators\n",
    "- Distinguish between near-term (1-2 years) and longer-term developments\n",
    "- Note any dependencies or barriers to adoption\n",
    "- Provide confidence levels for predictions\n",
    "- Cite evidence supporting timeline estimates\n",
    "\n",
    "ANSWER:\"\"\",\n",
    "    \n",
    "    # TRL template ONLY for queries that explicitly need it\n",
    "    \"trl_transition\": \"\"\"\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Assess technology maturity levels based on evidence in context\n",
    "- Identify which technologies are transitioning from research to application\n",
    "- Note key organizations driving commercialization\n",
    "- Estimate development timelines where evidence exists\n",
    "- Provide specific citations for maturity assessments\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "}\n",
    "\n",
    "def get_prompt_template(question):\n",
    "    \"\"\"Select appropriate template - TRL only for explicit maturity questions\"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # TRL definitions ONLY for queries 8-9 that explicitly need maturity assessment\n",
    "    if any(keyword in question_lower for keyword in ['trl', 'mature', 'transition', 'academy to application', 'commercial', 'moving from academy']):\n",
    "        return PROMPT_TEMPLATES[\"trl_transition\"]\n",
    "    \n",
    "    # Forecast and timing questions (query 6)\n",
    "    elif any(keyword in question_lower for keyword in ['next year', 'timing', 'forecast', 'likely to mature', 'when will']):\n",
    "        return PROMPT_TEMPLATES[\"forecast\"]\n",
    "    \n",
    "    # Trend and velocity questions (queries 3,7)\n",
    "    elif any(keyword in question_lower for keyword in ['trend', 'growing', 'latest', 'velocity', 'fastest']):\n",
    "        return PROMPT_TEMPLATES[\"trends\"]\n",
    "    \n",
    "    # Explanatory questions (queries 2,4)\n",
    "    elif any(keyword in question_lower for keyword in ['summarize', 'explain', 'pain points', 'use cases', 'how is']):\n",
    "        return PROMPT_TEMPLATES[\"explanatory\"]\n",
    "    \n",
    "    # Default for descriptive/factual questions (queries 1,5)\n",
    "    else:\n",
    "        return PROMPT_TEMPLATES[\"descriptive\"]\n",
    "\n",
    "print(\"‚úÖ Optimized prompt templates - TRL only for relevant queries\")\n",
    "print(\"‚úÖ Queries 1-7 use clean templates, 8-9 get TRL context when needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de134455",
   "metadata": {},
   "source": [
    "# Response Quality Setup\n",
    "\n",
    "**Why we're doing this:** \n",
    "Ensure answers are relevant and properly cite sources.\n",
    "\n",
    "**What we're doing:**\n",
    "\n",
    "- Formatting retrieved chunks as context - to-do\n",
    "- Adding source citations to responses - to-do\n",
    "- Setting temperature parameters for consistency - to-do\n",
    "\n",
    "**Files accessed:**\n",
    "/models/vector_index/ (FAISS index)\n",
    "/data/processed/chunked_corpus.json (for source metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
