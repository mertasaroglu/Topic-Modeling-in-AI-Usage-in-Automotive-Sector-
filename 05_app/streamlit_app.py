# AUTOMOTIVE TECH INTELLIGENCE - STREAMLIT APP
# Complete RAG interface with Query Expansion
# UPDATED FOR FAISS RETRIEVER WITH UNIVERSAL HYBRID SEARCH

import streamlit as st
import sys
import os
import importlib.util
import re  # Added for regex matching

def get_correct_paths():
    """Get absolute paths based on your exact folder structure"""
    current_dir = os.path.dirname(os.path.abspath(__file__))  # 05_app folder
    project_root = os.path.dirname(current_dir)  # innovation-intelligence-suite
    
    rag_components_path = os.path.join(project_root, '03_notebooks', 'rag_notebooks', 'rag_components')
    vector_index_path = os.path.join(project_root, '04_models', 'vector_index')
    
    return rag_components_path, vector_index_path, project_root

def import_your_components():
    """Import FAISS retriever with exact paths"""
    rag_components_path, _, _ = get_correct_paths()
    
    # Updated for FAISS retriever
    faiss_retriever_path = os.path.join(rag_components_path, 'faiss_retriever.py')
    
    if not os.path.exists(faiss_retriever_path):
        return None, f"FAISS Retriever not found at: {faiss_retriever_path}"
        
    try:
        if rag_components_path not in sys.path:
            sys.path.insert(0, rag_components_path)
        
        spec = importlib.util.spec_from_file_location("faiss_retriever", faiss_retriever_path)
        retriever_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(retriever_module)
        return retriever_module, None
    except Exception as e:
        return None, f"Error importing FAISS retriever: {str(e)}"

def import_query_expander():
    """Import the query expander module"""
    rag_components_path, _, _ = get_correct_paths()
    expander_path = os.path.join(rag_components_path, 'query_expander.py')
    
    if not os.path.exists(expander_path):
        # Create enhanced query expander
        with open(expander_path, 'w') as f:
            f.write('''
class QueryExpander:
    """Enhanced query expander for automotive domain."""
    
    def expand_query(self, query, use_llm=False):
        """Expand query with domain-specific variations."""
        variations = [query]
        query_lower = query.lower()
        
        # AUTOMOTIVE DOMAIN EXPANSION
        if any(word in query_lower for word in ['automotive', 'vehicle', 'car', 'truck']):
            variations.extend([
                'automobile',
                'mobility',
                'transportation',
                'autonomous vehicles',
                'connected cars'
            ])
        
        # AI DOMAIN EXPANSION
        if any(word in query_lower for word in ['ai', 'artificial intelligence', 'machine learning']):
            variations.extend([
                'machine learning',
                'deep learning',
                'neural networks',
                'algorithm',
                'intelligent systems'
            ])
        
        # STARTUP SPECIFIC EXPANSION
        if any(word in query_lower for word in ['startup', 'company', 'venture']):
            variations.extend([
                'emerging companies',
                'new businesses',
                'tech ventures',
                'entrepreneurship',
                'scale-up'
            ])
        
        # PATENT SPECIFIC EXPANSION
        if any(word in query_lower for word in ['patent', 'intellectual property', 'ip']):
            variations.extend([
                'intellectual property',
                'patents',
                'invention',
                'innovation protection',
                'IP rights'
            ])
        
        # RESEARCH SPECIFIC EXPANSION
        if any(word in query_lower for word in ['research', 'study', 'paper', 'academic']):
            variations.extend([
                'academic research',
                'scientific study',
                'scholarly article',
                'technical paper'
            ])
        
        # TREND SPECIFIC EXPANSION
        if any(word in query_lower for word in ['trend', 'forecast', 'future', 'emerging']):
            variations.extend([
                'emerging trends',
                'future developments',
                'market trends',
                'technological forecast'
            ])
        
        # MATURITY SPECIFIC EXPANSION
        if any(word in query_lower for word in ['trl', 'maturity', 'readiness', 'commercial']):
            variations.extend([
                'technology readiness',
                'development stage',
                'commercialization',
                'technology adoption'
            ])
        
        return list(set(variations))[:5]  # Return unique, max 5 variations
''')
    
    try:
        if rag_components_path not in sys.path:
            sys.path.insert(0, rag_components_path)
        
        spec = importlib.util.spec_from_file_location("query_expander", expander_path)
        expander_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(expander_module)
        return expander_module, None
    except Exception as e:
        return None, f"Error importing query expander: {str(e)}"

def setup_groq_client():
    """Your exact client setup from notebook 03"""
    try:
        from groq import Groq
        from dotenv import load_dotenv
        load_dotenv()
        
        api_key = os.getenv('GROQ_API_KEY')
        if not api_key:
            return None, "GROQ_API_KEY not found in environment variables"
        
        client = Groq(api_key=api_key)
        return client, None
    except ImportError:
        return None, "Groq package not installed. Run: pip install groq"
    except Exception as e:
        return None, f"Error setting up Groq client: {str(e)}"

def build_smart_prompt(question, context):
    """Enhanced prompt template with specific guidance for all query types"""
    question_lower = question.lower()
    
    # DETECT QUERY TYPE FOR TARGETED GUIDANCE
    is_startup_question = any(keyword in question_lower for keyword in 
                            ['startup', 'company', 'companies', 'venture', 'business', 'funding'])
    
    is_patent_question = any(keyword in question_lower for keyword in 
                           ['patent', 'intellectual property', 'ip', 'jurisdiction', 'ep', 'us', 'wo'])
    
    is_research_question = any(keyword in question_lower for keyword in 
                             ['research', 'study', 'paper', 'academic', 'scientific', 'methodology'])
    
    is_trend_question = any(keyword in question_lower for keyword in 
                          ['trend', 'forecast', 'future', 'emerging', 'development', 'innovation', 'pain point', 'challenge'])
    
    is_maturity_question = any(keyword in question_lower for keyword in 
                             ['trl', 'maturity', 'readiness', 'commercial', 'transition', 'stage'])
    
    is_technology_question = any(keyword in question_lower for keyword in 
                               ['technology', 'tech', 'system', 'solution', 'application', 'deployment', 'agent', 'agents'])
    
    # BUILD TARGETED GUIDANCE SECTIONS
    guidance_sections = []
    
    # PATENT GUIDANCE
    if is_patent_question:
        guidance_sections.append("""
üîç **PATENT QUERY GUIDANCE:**
1. **EXTRACT PATENT DETAILS**: Patent numbers, titles, inventors, assignees, jurisdictions
2. **ANALYZE JURISDICTIONS**: 
   - EP: European Patent Office (covers multiple countries)
   - US: United States Patent and Trademark Office
   - WO: World Intellectual Property Organization (international applications)
3. **IDENTIFY TECHNOLOGIES**: Specific automotive/AI technologies protected
4. **NOTE KEY DATES**: Filing dates, publication dates, grant dates when available
5. **ORGANIZE BY TYPE**: Group by jurisdiction or technology area
6. **SOURCE SPECIFICALLY**: Always cite patent database sources [Source: Automotive Technology Patents Database]
""")
    
    # STARTUP GUIDANCE
    if is_startup_question:
        guidance_sections.append("""
üöÄ **STARTUP QUERY GUIDANCE:**
1. **EXTRACT COMPANY NAMES**: All startup/company names mentioned
2. **INCLUDE DETAILS**: Location, founding year, funding stage, key technologies
3. **FOCUS ON DATABASES**: Prioritize information from startup-specific sources
4. **ORGANIZE CLEARLY**: Create numbered lists with consistent formatting
5. **HIGHLIGHT AI FOCUS**: Note AI applications in automotive context
6. **CITE PROPERLY**: Always include source names
""")
    
    # RESEARCH GUIDANCE
    if is_research_question:
        guidance_sections.append("""
üìö **RESEARCH QUERY GUIDANCE:**
1. **EXTRACT KEY FINDINGS**: Main conclusions, methodologies, results
2. **IDENTIFY AUTHORS & INSTITUTIONS**: Research teams and affiliations
3. **NOTE TECHNICAL DETAILS**: Specific algorithms, models, datasets used
4. **ASSESS NOVELTY**: Unique contributions or innovations mentioned
5. **CONNECT TO APPLICATIONS**: Practical automotive applications discussed
6. **ORGANIZE BY THEME**: Group related research findings together
""")
    
    # TREND/GUIDANCE
    if is_trend_question:
        guidance_sections.append("""
üìà **TREND/CHALLENGE GUIDANCE:**
1. **IDENTIFY KEY TRENDS/PAIN POINTS**: Major developments, challenges, or patterns
2. **EXTRACT VELOCITY INDICATORS**: Growth rates, adoption curves, investment trends
3. **NOTE DRIVERS & BARRIERS**: Factors enabling or hindering adoption
4. **HIGHLIGHT KEY PLAYERS**: Companies, institutions mentioned
5. **PROVIDE EXAMPLES**: Specific technologies or cases mentioned
6. **COMPARE SOURCES**: Note consistency or variations across different reports
""")
    
    # MATURITY GUIDANCE
    if is_maturity_question:
        guidance_sections.append("""
üéØ **TECHNOLOGY MATURITY GUIDANCE:**
1. **ASSESS TRL LEVELS**: Technology Readiness Levels 1-9 when mentioned
2. **IDENTIFY STAGE**: Research (TRL 1-4), Development (TRL 5-6), Commercial (TRL 7-9)
3. **NOTE TRANSITION POINTS**: Key milestones for advancement
4. **EXTRACT EVIDENCE**: Prototypes, pilots, deployments mentioned
5. **ANALYZE TIMELINES**: Expected development or adoption timelines
6. **PROVIDE SPECIFIC EXAMPLES**: Specific technologies and their maturity levels
""")
    
    # TECHNOLOGY/GUIDANCE
    if is_technology_question:
        guidance_sections.append("""
‚öôÔ∏è **TECHNOLOGY QUERY GUIDANCE:**
1. **EXTRACT SPECIFICS**: Technology names, versions, capabilities
2. **IDENTIFY APPLICATIONS**: How technologies are used in automotive context
3. **NOTE PERFORMANCE METRICS**: Speed, accuracy, efficiency improvements
4. **ASSESS INTEGRATION**: How technologies work together or integrate
5. **HIGHLIGHT INNOVATIONS**: Novel approaches or breakthroughs
6. **COMPARE ALTERNATIVES**: Different technology options mentioned
""")
    
    # GENERAL GUIDANCE FOR ALL QUERIES
    general_guidance = """
üìã **GENERAL ANSWER GUIDELINES:**
1. **BE SPECIFIC**: Use exact names, numbers, dates from context
2. **BE COMPREHENSIVE**: Cover all relevant aspects of the question
3. **BE STRUCTURED**: Use clear organization (numbered lists, sections)
4. **BE ACCURATE**: Only use information from the provided context
5. **CITE SOURCES**: For each key point, include [Source: Name]
6. **ACKNOWLEDGE LIMITATIONS**: If information is incomplete, state what's missing
"""
    
    # COMBINE ALL GUIDANCE
    targeted_guidance = "\n\n".join(guidance_sections)
    
    prompt = f"""
CONTEXT:
{context}

USER QUESTION:
{question}

ANALYSIS INSTRUCTIONS:
You are an automotive technology intelligence analyst. Your task is to provide detailed, accurate answers based strictly on the context provided.

{targeted_guidance}

{general_guidance}

FORMAT REQUIREMENTS:
- Use **bold** for company names, technology names, patent numbers
- Use numbered lists for multiple items (e.g., 1., 2., 3.)
- Use bullet points for sub-items within descriptions
- Include specific metrics (percentages, amounts, dates) when available
- Group related information together (e.g., by technology, by company, by region)

ANSWER STRUCTURE:
1. Direct answer to the main question
2. Supporting details with specific examples
3. Source citations for each key point
4. Summary or implications if relevant

ANSWER:
"""
    return prompt

def determine_source_count(question):
    """Dynamic source counting based on question type"""
    question_lower = question.lower()
    
    # Complex questions need more sources
    if any(keyword in question_lower for keyword in ['summarize', 'comprehensive', 'overall', 'complete', 'latest']):
        return 5
    # List questions need more sources for coverage
    elif any(keyword in question_lower for keyword in ['list', 'which', 'what are', 'show all', 'show me']):
        return 5
    # Specific questions can use fewer sources
    elif any(keyword in question_lower for keyword in ['specific', 'exact', 'precise', 'detailed']):
        return 3
    # Default for most questions
    else:
        return 4

def format_source_name(source_file):
    """Enhanced file name formatting with icons"""
    name_mapping = {
        # Automotive Papers
        'a_benchmark_framework_for_AL_models_in_automotive_aerodynamics.txt': 'üìä AI in Automotive Aerodynamics Research',
        'AL_agents_in_engineering_design_a_multiagent_framework_for_aesthetic_and_aerodynamic_car_design.txt': 'ü§ñ AI Agents in Car Design Research',
        'automating_automotive_software_development_a_synergy_of_generative_AL_and_formal_methods.txt': '‚öôÔ∏è AI for Automotive Software Development',
        'automotive-software-and-electronics-2030-full-report.txt': 'üìà Automotive Software 2030 Report',
        'drive_disfluency-rich_synthetic_dialog_data_generation_framework_for_intelligent_vehicle_environments.txt': 'üó£Ô∏è AI Dialogue Systems for Vehicles',
        'Embedded_acoustic_intelligence_for_automotive_systems.txt': 'üîä Acoustic AI for Automotive Systems',
        'enhanced_drift_aware_computer_vision_achitecture_for_autonomous_driving.txt': 'üëÅÔ∏è Computer Vision for Autonomous Driving',
        'Gen_AL_in_automotive_applications_challenges_and_opportunities_with_a_case_study_on_in-vehicle_experience.txt': 'üé® Generative AI in Automotive Applications',
        'generative_AL_for_autonomous_driving_a_review.txt': 'üìö Generative AI for Autonomous Driving Review',
        'leveraging_vision_language_models_for_visual_grounding_and_analysis_of_automative_UI.txt': 'üëÅÔ∏èüó£Ô∏è Vision-Language Models for Automotive UI',
        
        # Tech Reports
        'bog_ai_value_2025.txt': 'üè¢ BCG: AI Value Creation 2025',
        'mckinsey_tech_trends_2025.txt': 'üìä McKinsey Technology Trends 2025',
        'wef_emerging_tech_2025.txt': 'üåç WEF: Emerging Technologies 2025',
        
        # Processed Files
        'autotechinsight_startups_processed.txt': 'üöÄ AutoTechInsight Automotive Startup Profiles & Tracker',
        'seedtable_startups_processed.txt': 'üìà Seedtable Best Automotive Industry Startups to Watch in 2025',
        'automotive_papers_processed.txt': 'üìö Automotive Research Papers Database',
        'automotive_patents_processed.txt': 'üìú Automotive Technology Patents Database',
        
        # Generic fallbacks
        'startup': 'üöÄ Startup Database',
        'patent': 'üìú Patent Database',
        'paper': 'üìö Research Database',
        'report': 'üìä Industry Report',
    }
    
    # Try exact match first
    if source_file in name_mapping:
        return name_mapping[source_file]
    
    # Try partial matching
    source_lower = source_file.lower()
    for key, value in name_mapping.items():
        if key in source_lower:
            return value
    
    # Default formatting
    return source_file.replace('.txt', '').replace('_', ' ').title()

# Initialize components with lazy loading
@st.cache_resource
def initialize_rag_system():
    """Initialize all RAG components using exact paths - UPDATED FOR FAISS"""
    rag_components_path, vector_index_path, project_root = get_correct_paths()
    
    # Check if vector index exists - updated check for FAISS files
    if not os.path.exists(vector_index_path):
        return None, None, None, f"Vector index not found at: {vector_index_path}"
    
    # Check for FAISS files specifically
    faiss_files = ['faiss_index.bin', 'texts.pkl', 'metadata.pkl']
    missing_files = []
    for file in faiss_files:
        if not os.path.exists(os.path.join(vector_index_path, file)):
            missing_files.append(file)
    
    if missing_files:
        return None, None, None, f"FAISS files missing: {', '.join(missing_files)}. Did you run the FAISS embedding creation?"
    
    # Import FAISS retriever
    retriever_module, retriever_error = import_your_components()
    if retriever_error:
        return None, None, None, retriever_error
    
    # Import query expander
    expander_module, expander_error = import_query_expander()
    if expander_error and "placeholder" not in expander_error:
        print(f"Note: Query expander not fully available: {expander_error}")
    
    # Setup Groq client
    groq_client, groq_error = setup_groq_client()
    if groq_error:
        return None, None, None, groq_error
    
    # Initialize FAISS retriever
    try:
        retriever = retriever_module.FAISSRetriever(vector_index_path)
        
        # Initialize query expander if module loaded
        query_expander = None
        if expander_module and not expander_error:
            try:
                query_expander = expander_module.QueryExpander()
                print("‚úÖ Enhanced query expander initialized")
            except Exception as e:
                print(f"‚ö†Ô∏è Query expander init warning: {e}")
                query_expander = None
        
        return retriever, groq_client, query_expander, None
        
    except Exception as e:
        return None, None, None, f"Error initializing FAISS retriever: {str(e)}"

def analyze_question_type(question):
    """Analyze question to determine optimal retrieval strategy"""
    question_lower = question.lower()
    
    # Define question type characteristics
    question_type = {
        'is_startup_query': any(keyword in question_lower for keyword in 
                               ['startup', 'company', 'companies', 'venture', 'business', 'funding']),
        'is_patent_query': any(keyword in question_lower for keyword in 
                              ['patent', 'intellectual property', 'ip', 'jurisdiction', 'ep', 'us', 'wo']),
        'is_research_query': any(keyword in question_lower for keyword in 
                                ['research', 'study', 'paper', 'academic', 'method', 'experiment']),
        'is_trend_query': any(keyword in question_lower for keyword in 
                             ['trend', 'forecast', 'future', 'emerging', 'development', 'innovation', 'pain point', 'challenge']),
        'is_maturity_query': any(keyword in question_lower for keyword in 
                                ['trl', 'maturity', 'readiness', 'commercial', 'stage', 'transition']),
        'is_technology_query': any(keyword in question_lower for keyword in 
                                  ['technology', 'tech', 'system', 'solution', 'application', 'algorithm', 'agent', 'agents']),
        'is_list_query': any(keyword in question_lower for keyword in 
                            ['list', 'which', 'what are', 'name', 'show me']),
        'is_summary_query': any(keyword in question_lower for keyword in 
                               ['summarize', 'overview', 'explain', 'describe', 'what is']),
    }
    
    # Determine optimal thresholds based on question type
    if question_type['is_startup_query'] or question_type['is_list_query']:
        semantic_threshold = 0.15  # Lower for broad coverage
        keyword_threshold = 0.1    # Very low for keyword matching
        k_semantic = 4             # More semantic results
        k_keyword = 2              # Fewer keyword results
    elif question_type['is_patent_query']:
        semantic_threshold = 0.25  # Moderate for technical queries
        keyword_threshold = 0.15   # Low for technical terms
        k_semantic = 4
        k_keyword = 2
    elif question_type['is_research_query']:
        semantic_threshold = 0.3   # Standard for academic content
        keyword_threshold = 0.2    # Standard keyword matching
        k_semantic = 3
        k_keyword = 2
    else:
        semantic_threshold = 0.25  # Balanced for general queries
        keyword_threshold = 0.15   # Standard keyword matching
        k_semantic = 3
        k_keyword = 2
    
    return question_type, semantic_threshold, keyword_threshold, k_semantic, k_keyword

def get_targeted_keyword_queries(question, question_type):
    """Generate targeted keyword queries based on question type"""
    question_lower = question.lower()
    keyword_queries = []
    
    # STARTUP-RELATED KEYWORDS
    if question_type['is_startup_query']:
        keyword_queries.extend([
            "automotive startup",
            "AI company automotive",
            "autonomous vehicle company",
            "electric vehicle startup",
            "mobility tech company",
            "car technology startup",
            "vehicle AI startup",
            "automotive venture capital",
            "tech startup funding",
            "emerging automotive companies"
        ])
    
    # PATENT-RELATED KEYWORDS
    if question_type['is_patent_query']:
        keyword_queries.extend([
            "automotive patent",
            "AI patent automotive",
            "vehicle technology patent",
            "intellectual property automotive",
            "patent EP automotive",
            "patent US automotive",
            "patent WO automotive",
            "automotive invention",
            "vehicle innovation patent",
            "autonomous driving patent"
        ])
    
    # RESEARCH-RELATED KEYWORDS
    if question_type['is_research_query']:
        keyword_queries.extend([
            "automotive research",
            "AI study automotive",
            "vehicle technology research",
            "autonomous driving study",
            "electric vehicle research",
            "automotive AI paper",
            "vehicle system research",
            "mobility technology study",
            "automotive engineering research",
            "intelligent vehicle study"
        ])
    
    # TREND/CHALLENGE-RELATED KEYWORDS
    if question_type['is_trend_query']:
        keyword_queries.extend([
            "automotive trend",
            "AI trend automotive",
            "vehicle technology trend",
            "emerging automotive technology",
            "future of automotive",
            "automotive innovation trend",
            "mobility future trend",
            "vehicle market trend",
            "automotive industry trend",
            "tech trend automotive",
            "automotive challenge",
            "AI challenge automotive",
            "vehicle technology barrier",
            "automotive adoption barrier"
        ])
    
    # MATURITY-RELATED KEYWORDS
    if question_type['is_maturity_query']:
        keyword_queries.extend([
            "technology readiness automotive",
            "TRL automotive",
            "maturity automotive technology",
            "commercialization automotive",
            "development stage automotive",
            "automotive technology adoption",
            "vehicle tech readiness",
            "automotive innovation maturity",
            "scaling automotive technology",
            "deployment automotive AI"
        ])
    
    # TECHNOLOGY/AGENT-RELATED KEYWORDS
    if question_type['is_technology_query']:
        # Extract technology terms from question
        tech_terms = re.findall(r'\b[A-Z][a-z]+\b', question)
        for term in tech_terms[:3]:  # Use first 3 capitalized terms
            if len(term) > 3 and term.lower() not in ['ai', 'automotive', 'vehicle', 'car']:  # Avoid common words
                keyword_queries.append(f"{term} automotive")
                keyword_queries.append(f"{term} vehicle")
        
        # Special handling for AI agents
        if 'agent' in question_lower or 'agents' in question_lower:
            keyword_queries.extend([
                "AI agent automotive",
                "intelligent agent vehicle",
                "autonomous agent system",
                "multi-agent system automotive",
                "agent-based automotive"
            ])
        
        keyword_queries.extend([
            "automotive technology",
            "vehicle system",
            "car technology",
            "automotive solution",
            "vehicle application",
            "automotive AI system",
            "intelligent vehicle technology",
            "connected car technology",
            "autonomous driving system",
            "electric vehicle technology"
        ])
    
    # GENERAL AUTOMOTIVE/AI KEYWORDS (for all queries)
    keyword_queries.extend([
        "automotive AI",
        "vehicle artificial intelligence",
        "car machine learning",
        "autonomous vehicle",
        "electric vehicle",
        "connected car",
        "smart mobility",
        "intelligent transportation"
    ])
    
    # Remove duplicates and limit
    return list(set(keyword_queries))[:8]  # Max 8 keyword queries

def universal_hybrid_retrieval(question, retriever, query_expander=None, k=4):
    """
    üöÄ UNIVERSAL HYBRID RETRIEVAL: Optimized for all query types
    
    This approach:
    1. Analyzes question type
    2. Uses targeted semantic search with query expansion
    3. Adds keyword searches optimized for the question type
    4. Ensures relevant document types are included
    5. Removes duplicates and optimizes ranking
    """
    all_results = []
    
    # STEP 1: ANALYZE QUESTION TYPE
    question_type, semantic_threshold, keyword_threshold, k_semantic, k_keyword = analyze_question_type(question)
    
    # STEP 2: SEMANTIC SEARCH WITH QUERY EXPANSION
    if query_expander:
        try:
            expanded_queries = query_expander.expand_query(question, use_llm=False)
            if not expanded_queries:
                expanded_queries = [question]
        except:
            expanded_queries = [question]
    else:
        expanded_queries = [question]
    
    # Semantic search with expanded queries
    for query in expanded_queries[:3]:  # Use first 3 expanded queries
        try:
            semantic_results = retriever.retrieve_with_sources(
                query, 
                k=k_semantic, 
                threshold=semantic_threshold
            )
            all_results.extend(semantic_results)
        except Exception as e:
            continue
    
    # STEP 3: TARGETED KEYWORD SEARCH
    keyword_queries = get_targeted_keyword_queries(question, question_type)
    
    for keyword_query in keyword_queries[:5]:  # Use first 5 keyword queries
        try:
            keyword_results = retriever.retrieve_with_sources(
                keyword_query,
                k=k_keyword,
                threshold=keyword_threshold
            )
            
            # Filter to ensure relevance
            filtered_results = []
            for result in keyword_results:
                content = result.get('text', result.get('content', '')).lower()
                # Check if result contains relevant terms
                if any(term in content for term in keyword_query.split()[:2]):
                    filtered_results.append(result)
            
            all_results.extend(filtered_results)
        except Exception as e:
            continue
    
    # STEP 4: DOCUMENT TYPE ENSURANCE
    # Ensure relevant document types are included based on question type
    required_doc_types = []
    
    if question_type['is_startup_query']:
        required_doc_types.extend(['startup', 'seedtable', 'autotech'])
    
    if question_type['is_patent_query']:
        required_doc_types.extend(['patent', 'lens'])
    
    if question_type['is_research_query']:
        required_doc_types.extend(['paper', 'research', 'study', 'academic'])
    
    # Check if we have required document types
    for doc_type in required_doc_types[:2]:  # Check first 2 required types
        has_type = False
        for result in all_results:
            source_file = result.get('source_file', '').lower()
            if doc_type in source_file:
                has_type = True
                break
        
        # If missing, try to find documents of this type
        if not has_type:
            try:
                type_results = retriever.retrieve_with_sources(
                    doc_type,
                    k=1,
                    threshold=0.05  # Extremely low threshold
                )
                all_results.extend(type_results)
            except:
                pass
    
    # STEP 5: DEDUPLICATION AND RANKING
    # Remove duplicates
    unique_results = []
    seen_content = set()
    
    for result in all_results:
        content = result.get('text', result.get('content', ''))
        content_start = content[:250]  # Use first 250 chars for deduplication
        source = result.get('source_file', 'unknown')
        signature = f"{source}:{content_start}"
        
        if signature not in seen_content:
            seen_content.add(signature)
            unique_results.append(result)
    
    # Enhanced ranking: prioritize by relevance to question type
    def calculate_relevance_score(result, question_type):
        """Calculate enhanced relevance score based on question type"""
        base_score = result.get('similarity_score', 0)
        source_file = result.get('source_file', '').lower()
        
        # Type matching bonus
        type_bonus = 0
        
        if question_type['is_startup_query'] and any(keyword in source_file for keyword in ['startup', 'seedtable', 'autotech']):
            type_bonus += 0.3
        
        if question_type['is_patent_query'] and any(keyword in source_file for keyword in ['patent', 'lens']):
            type_bonus += 0.3
        
        if question_type['is_research_query'] and any(keyword in source_file for keyword in ['paper', 'research', 'study']):
            type_bonus += 0.2
        
        return base_score + type_bonus
    
    # Sort by enhanced relevance score
    unique_results.sort(key=lambda x: calculate_relevance_score(x, question_type), reverse=True)
    
    # Return top k results
    return unique_results[:k]

def retrieve_with_expansion(question, retriever, query_expander=None, k=4):
    """
    Main retrieval function - uses universal hybrid approach
    """
    return universal_hybrid_retrieval(question, retriever, query_expander, k)

def ask_question(question, retriever, groq_client, query_expander=None):
    """Enhanced RAG pipeline with universal hybrid retrieval"""
    try:
        k = determine_source_count(question)
        
        # Use universal hybrid retrieval
        retrieved_data = retrieve_with_expansion(
            question, 
            retriever, 
            query_expander=query_expander, 
            k=k
        )
        
        if not retrieved_data:
            return {
                'answer': "I couldn't find relevant information in our knowledge base for this specific question. Try asking about:\n\n‚Ä¢ Automotive AI startups and companies\n‚Ä¢ Automotive technology patents and IP\n‚Ä¢ AI research in autonomous driving\n‚Ä¢ Technology trends and challenges in the automotive industry\n‚Ä¢ Technology maturity and readiness levels",
                'sources': [],
                'success': True,
                'source_count': k
            }
        
        # Build enhanced context with source information
        context_parts = []
        for i, item in enumerate(retrieved_data):
            content = item.get('text', item.get('content', ''))
            source_file = item.get('source_file', 'unknown')
            
            # Get doc_type with fallbacks
            doc_type = item.get('doc_type', 'document')
            if doc_type == 'document' and 'metadata' in item:
                doc_type = item['metadata'].get('doc_type', item['metadata'].get('type', 'document'))
            
            readable_name = format_source_name(source_file)
            similarity = item.get('similarity_score', 0)
            
            context_parts.append(f"--- DOCUMENT {i+1} ---")
            context_parts.append(f"Source: {readable_name}")
            context_parts.append(f"Type: {doc_type}")
            context_parts.append(f"Relevance Score: {similarity:.3f}")
            context_parts.append(f"Content:\n{content}")
            context_parts.append("")  # Empty line for separation
        
        context = "\n".join(context_parts)
        prompt = build_smart_prompt(question, context)
        
        # Adjust tokens based on context length
        max_tokens = 800 if len(context) > 3000 else 600
        
        response = groq_client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=0.3
        )
        
        answer = response.choices[0].message.content
        
        return {
            'answer': answer,
            'sources': retrieved_data,
            'success': True,
            'source_count': k
        }
        
    except Exception as e:
        return {
            'answer': f"I encountered an error while processing your question: {str(e)}",
            'sources': [],
            'success': False
        }

# STREAMLIT UI
def main():
    st.set_page_config(
        page_title="INNOVATION INTELLIGENCE SUITE", 
        page_icon="üöó", 
        layout="wide"
    )
    
    st.title("INNOVATION INTELLIGENCE SUITE")
    st.markdown("Ask questions about latest tech trends in the automotive industry, including patents and startups.")
    
    # Initialize system with session state for persistence
    if 'rag_initialized' not in st.session_state:
        with st.spinner("Loading your RAG system with FAISS..."):
            st.session_state.retriever, st.session_state.groq_client, st.session_state.query_expander, error = initialize_rag_system()
            
            if error:
                st.session_state.rag_initialized = False
                st.error(f"‚ùå System initialization failed")
                
                # User-friendly error messages with solutions
                if "Groq package not installed" in error:
                    st.error("**Missing Dependency**")
                    st.info("""
                    **Solution:** Install the Groq package:
                    ```bash
                    pip install groq
                    ```
                    Then restart the Streamlit app.
                    """)
                elif "GROQ_API_KEY" in error:
                    st.error("**API Key Missing**")
                    st.info("""
                    **Solution:** Add your Groq API key to the `.env` file in the project root:
                    ```
                    GROQ_API_KEY=your_actual_api_key_here
                    ```
                    """)
                elif "Vector index not found" in error or "FAISS files missing" in error:
                    st.error("**Knowledge Base Missing**")
                    st.info("""
                    **Solution:** Generate the FAISS vector index first by running the FAISS embedding creation code in notebook 02.
                    The FAISS index should be in `04_models/vector_index/` with these files:
                    - faiss_index.bin
                    - texts.pkl  
                    - metadata.pkl
                    - embeddings.npy
                    """)
                elif "FAISS Retriever not found" in error:
                    st.error("**FAISS Retriever Missing**")
                    st.info("""
                    **Solution:** Make sure `faiss_retriever.py` exists in `rag_components/` folder.
                    Also install required packages:
                    ```bash
                    pip install faiss-cpu fastembed
                    ```
                    """)
                elif "Error importing FAISS retriever" in error:
                    st.error("**Import Error**")
                    st.info("""
                    **Solution:** Check that `faiss_retriever.py` has correct imports:
                    ```python
                    import faiss
                    from fastembed import TextEmbedding
                    ```
                    Install missing packages with:
                    ```bash
                    pip install faiss-cpu fastembed
                    ```
                    """)
                else:
                    st.error(f"**Error:** {error}")
                    
            elif st.session_state.retriever and st.session_state.groq_client:
                st.session_state.rag_initialized = True
                if st.session_state.query_expander:
                    st.success("System ready.")
                else:
                    st.success("‚úÖ FAISS RAG system ready! (Query expansion not available)")
            else:
                st.session_state.rag_initialized = False
                st.error("‚ùå Failed to initialize RAG system")
    
    # Only show query interface if system is initialized
    if not st.session_state.get('rag_initialized', False):
        st.warning("Please fix the initialization issues above to use the query system.")
        
        # Show debug info in expander
        with st.expander("üîß Technical Details"):
            rag_path, vector_path, project_root = get_correct_paths()
            st.write(f"**Project Root:** `{project_root}`")
            st.write(f"**RAG Components Path:** `{rag_path}`")
            st.write(f"**Vector Index Path:** `{vector_path}`")
            st.write(f"**faiss_retriever.py exists:** {os.path.exists(os.path.join(rag_path, 'faiss_retriever.py'))}")
            st.write(f"**vector_index exists:** {os.path.exists(vector_path)}")
            
            # Check FAISS files
            if os.path.exists(vector_path):
                faiss_files = ['faiss_index.bin', 'texts.pkl', 'metadata.pkl']
                for file in faiss_files:
                    file_path = os.path.join(vector_index_path, file)
                    st.write(f"**{file} exists:** {os.path.exists(file_path)}")
            
            st.write(f"**query_expander.py exists:** {os.path.exists(os.path.join(rag_path, 'query_expander.py'))}")
            
            # Check .env file
            env_path = os.path.join(project_root, '.env')
            if os.path.exists(env_path):
                with open(env_path, 'r') as f:
                    env_content = f.read()
                st.write(f"**.env exists:** Yes")
                st.write(f"**GROQ_API_KEY in .env:** {'GROQ_API_KEY' in env_content}")
            else:
                st.write(f"**.env exists:** No")
        
        return
    
    # Query interface (only shown when system is ready)
    st.success("System ready.")
    
    # Initialize question input in session state if not exists
    if 'question_input' not in st.session_state:
        st.session_state.question_input = ""
    
    # Initialize button flags if not exists
    if 'research_clicked' not in st.session_state:
        st.session_state.research_clicked = False
    if 'patents_clicked' not in st.session_state:
        st.session_state.patents_clicked = False
    if 'startups_clicked' not in st.session_state:
        st.session_state.startups_clicked = False
    if 'trends_clicked' not in st.session_state:
        st.session_state.trends_clicked = False
    if 'agents_clicked' not in st.session_state:
        st.session_state.agents_clicked = False
    if 'maturity_clicked' not in st.session_state:
        st.session_state.maturity_clicked = False
    
    # Check for button clicks BEFORE creating the text input
    if st.session_state.research_clicked:
        st.session_state.question_input = "Summarize the latest AI research on autonomous driving vehicles."
        st.session_state.research_clicked = False
    elif st.session_state.patents_clicked:
        st.session_state.question_input = "Show me recent patents on AI for automotive vehicles."
        st.session_state.patents_clicked = False
    elif st.session_state.startups_clicked:
        st.session_state.question_input = "Which startups work on automotive and autonomous driving?"
        st.session_state.startups_clicked = False
    elif st.session_state.trends_clicked:
        st.session_state.question_input = "What are the key challenges and pain points in automotive AI adoption?"
        st.session_state.trends_clicked = False
    elif st.session_state.agents_clicked:
        st.session_state.question_input = "Summarize latest tech trends in development of AI agents."
        st.session_state.agents_clicked = False
    elif st.session_state.maturity_clicked:
        st.session_state.question_input = "Which automotive technologies are reaching commercial maturity in the next 12 months?"
        st.session_state.maturity_clicked = False
    
    # Query input - NOW this comes AFTER button checks
    question = st.text_input(
        "üí¨ Your question:",
        value=st.session_state.question_input,
        placeholder="e.g., Which startups work on AI for automotive?",
        key="question_input"
    )
    
    # Pre-defined query buttons
    st.subheader("üìã Example Questions")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üî¨ Latest AI Research", use_container_width=True, key="research_btn"):
            st.session_state.research_clicked = True
            st.rerun()
        if st.button("üìú Automotive Patents", use_container_width=True, key="patents_btn"):
            st.session_state.patents_clicked = True
            st.rerun()
        if st.button("üöÄ Startups in AI Automotive", use_container_width=True, key="startups_btn"):
            st.session_state.startups_clicked = True
            st.rerun()
    
    with col2:
        if st.button("üìà Industry Pain Points", use_container_width=True, key="trends_btn"):
            st.session_state.trends_clicked = True
            st.rerun()
        if st.button("ü§ñ AI Agents Development", use_container_width=True, key="agents_btn"):
            st.session_state.agents_clicked = True
            st.rerun()
        if st.button("üéØ Tech Maturity", use_container_width=True, key="maturity_btn"):
            st.session_state.maturity_clicked = True
            st.rerun()
    
    # Process question
    if question:
        with st.spinner("üîç Searching documents and generating answer..."):
            result = ask_question(
                question, 
                st.session_state.retriever, 
                st.session_state.groq_client,
                query_expander=st.session_state.query_expander
            )
        
        # Display results
        st.subheader("üìù Answer")
        st.write(result['answer'])
            
        # Display sources if available
        if result['sources']:
            st.subheader(f"üìö Sources ({len(result['sources'])} documents)")
            for i, source in enumerate(result['sources']):
                readable_name = format_source_name(source['source_file'])
                similarity = source.get('similarity_score', 0)
                
                with st.expander(f"üìÑ {readable_name} (Relevance: {similarity:.3f})"):
                    # Handle both 'text' (FAISS) and 'content' (old) field names
                    content = source.get('text', source.get('content', ''))
                    st.write(content)
                    
                    # Show metadata if available
                    if 'metadata' in source:
                        st.caption(f"**Metadata:** {source['metadata']}")
        
        st.markdown("---")
        st.caption("Powered by FAISS RAG + Universal Hybrid Search + Groq/Llama | Innovation Intelligence Suite")

if __name__ == "__main__":
    main()