TECH REPORT: AI_agents_in_engineering_design_a_multiagent_framework_for_aesthetic_and_aerodynamic_car_design
SOURCE: AI_agents_in_engineering_design_a_multiagent_framework_for_aesthetic_and_aerodynamic_car_design.pdf
============================================================

## KEY SECTIONS ##

### EXECUTIVE SUMMARY ###
significantly impact the overall agreement. This result highlights
themodelâ€™scapabilityinproducingrealisticaerodynamicpredic-
tions, reinforcing its applicability in engineering simulations.
Finally, the results in Table 3 demonstrate the simulation
agentâ€™s capability to accurately predict differences in drag coef-
ficients (Î”ğ¶ğ·) between various car configurations compared to
CFD simulations. For instance, the predicted drag difference
between the FD-FS and FD-N configurations is 0.066, closely
matchingtheCFDvalueof0.065. Similarly,thesimulationagent
correctly predicts the drag difference between the E-FS and E-N
configurations, with only minor deviations of 0.001. The largest
discrepancy appears in the N-FS configuration, where the pre-

--------------------------------------------------

### METHODOLOGY ###
[32] Canny, John. â€œA Computational Approach to Edge Detec-
tion.â€IEEE Transactions on Pattern Analysis and Machine
Intelligence Vol. 8 No. 6 (1986): pp. 679â€“698.
[33] Vinker, Yael, Pajouheshgar, Ehsan, Aberman, Kfir, Cohen-
Or, Daniel, Lischinski, Dani and Bermano, Amit H. â€œCLI-
Passo: Semantically-AwareObjectSketching.â€ Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition : pp. 22147â€“22156. 2022.
[34] Radford, Alec, Kim, Jong Wook, Hallacy, Chris, Ramesh,
Aditya, Goh, Gabriel, Agarwal, Sandhini, Sastry, Girish,
Askell, Amanda, Mishkin, Pamela, Clark, Jack, Krueger,
Gretchen and Sutskever, Ilya. â€œLearning transferable visual
models from natural language supervision.â€ International
Conference on Machine Learning : pp. 8748â€“8763. 2021.
PMLR.
[35] Frans, Kevin, Cheng, Chin-Yi, Fang, Ziqi, Gao, Yitong,
Nguyen, Jason Yim, Qian, Lei, Zhang, Feng and Hamilton,
William.â€œCLIPDraw: ExploringText-to-DrawingSynthesis
via Language-Image Encoders.â€ (2021). URL 2106.14843.
[36] Rombach, Robin, Blattmann, Andreas, Lorenz, Dominik,
Esser, Patrick and Ommer, BjÃ¶rn. â€œHigh-Resolution Im-
age Synthesis with Latent Diffusion Models.â€ Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (2022).
[37] Morita, Hayata, Shintani, Kohei, Yuan, Chenyang and Per-
menter, Frank. â€œVehicleSDF: A 3D generative model for
constrained engineering design via surrogate modeling.â€
arXiv preprint arXiv:2410.18986 (2024).
[38] Miao, JiangDong, Ikeda, Tatsuya, Raytchev, Bisser, Mi-
zoguchi, Ryota, Hiraoka, Takenori, Nakashima, Takuji,
Shimizu,Keigo,Higaki,ToruandKaneda,Kazufumi. â€œMa-
nipulating Vehicle 3D Shapes through Latent Space Edit-
ing.â€arXiv preprint arXiv:2410.23931 (2024).
[39] Lorensen,WilliamEandCline,HarveyE.â€œMarchingcubes:
Ahighresolution3Dsurfaceconstructionalgorithm.â€ Semi-
nalgraphics: pioneeringeffortsthatshapedthefield (1998):
pp. 347â€“353.[40] He,Kaiming,Zhang,Xiangyu,Ren,ShaoqingandSun,Jian.
â€œDeepresiduallearningforimagerecognition.â€ Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition : pp. 770â€“778. 2016.
[41] Pandey, Sandeep, Xu, Ran, Wang, Wenkang and Chu, Xu.
â€œOpenFOAMGPT: A retrieval-augmented large language
model (LLM) agent for OpenFOAM-based computational

--------------------------------------------------

### INTRODUCTION ###
are then post-processed to remove the background using mor-
phological operations and contour-based segmentation. The final
outputs, along with metadata including the applied prompts, are

--------------------------------------------------

### TECHNOLOGY CATALOG ###
fluid dynamics.â€ Physics of Fluids Vol. 37 No. 3 (2025).
[42] Xu, Leidong, Mohaddes, Danyal and Wang, Yi. â€œLLM
Agent for Fire Dynamics Simulations.â€ arXiv preprint
arXiv:2412.17146 (2024).
[43] Chen, Yuxuan, Zhu, Xu, Zhou, Hua and Ren, Zhuyin.
â€œMetaOpenFOAM: an LLM-based multi-agent framework
for CFD.â€ arXiv preprint arXiv:2407.21320 (2024).
[44] OpenFOAM Foundation. Meshing with snappyHexMesh
(2023). URL https://www.openfoam.com/documentation/
guides/latest/doc/guide-meshing-snappyhexmesh.html. Ac-
cessed: 2024-06-05.
[45] OpenAI. â€œGPT-3.5 Turbo.â€ https://platform.openai.com/
docs/models/gpt-3-5-turbo (2023). Last Accessed: 2024-
03-16.

--------------------------------------------------

### STRATEGIC OUTLOOK ###
capabilities highlight the transformative potential of integrated
AI-driven methods in engineering design workflows, encourag-
ing further research and practical adoption across academia and
industry.

--------------------------------------------------

### REFERENCES ###
electric vehicle under female aesthetic preferences.â€ Ad-
vanced Engineering Informatics Vol. 62 (2024): p. 102571.
[22] Ananthan, Vidyasagar, Ashton, Neil, Chadwick, Nate,
Lizarraga, Mariano, Maddix, Danielle, Maheswaran,
Satheesh, Moreno, Pablo Hermoso, Shabestari, Parisa M,
Sovani, Sandeep, Subramanian, Shreyas et al. â€œMachine
learning for road vehicle aerodynamics.â€ Technical report
no. SAE Technical Paper. 2024.[23] Arechiga, Nikos, Permenter, Frank, Song, Binyang and
Yuan, Chenyang. â€œDrag-guided diffusion models for ve-
hicle image generation.â€ arXiv preprint arXiv:2306.09935
(2023).
[24] Radhakrishnan, Sreedhar, Bharadwaj, Varun, Manjunath,
Varun and Srinath, Ramamoorthy. â€œCreative intelligenceâ€“
automatingcardesignstudiowithgenerativeadversarialnet-
works(GAN).â€ InternationalCross-DomainConferencefor
MachineLearningandKnowledgeExtraction : pp.160â€“175.
2018. Springer.
[25] Edwards, Kristen M, Man, Brandon and Ahmed, Faez.
â€œSketch2Prototype: rapidconceptualdesignexplorationand
prototypingwithgenerativeAI.â€ ProceedingsoftheDesign
SocietyVol. 4 (2024): pp. 1989â€“1998.
[26] Ocker, Felix, Menzel, Stefan, Sadik, Ahmed and Rios, Thi-
ago. â€œFromIdeatoCAD:ALanguageModel-DrivenMulti-
Agent System for Collaborative Design.â€ arXiv preprint
arXiv:2503.04417 (2025).
[27] Park, Jeong Joon, Florence, Peter, Straub, Julian, New-
combe,RichardandLovegrove,Steven.â€œDeepsdf: Learning
continuous signed distance functions for shape representa-
tion.â€ProceedingsoftheIEEE/CVFconferenceoncomputer
vision and pattern recognition : pp. 165â€“174. 2019.
[28] Qi, Charles R, Su, Hao, Mo, Kaichun and Guibas,
Leonidas J. â€œPointnet: Deep learning on point sets for 3d
classification and segmentation.â€ Proceedings of the IEEE
conference on computer vision and pattern recognition : pp.
652â€“660. 2017.
[29] Wang, Yue, Sun, Yongbin, Liu, Ziwei, Sarma, Sanjay E,
Bronstein, Michael M and Solomon, Justin M. â€œDynamic
graph cnn for learning on point clouds.â€ ACM Transactions

--------------------------------------------------

## FULL REPORT CONTENT ##

AI Agents in Engineering Design: A Multi-Agent Framework for Aesthetic and Aerodynamic Car Design
Mohamed Elrefaieâˆ—
Department of Mechanical
Engineering, Massachusetts
Institute of Technology, Cambridge,
MA, USAJanet Qian
Department of Electrical
Engineering and Computer Science,
Massachusetts Institute of
Technology, Cambridge, MA, USARaina Wu
Department of Mechanical
Engineering, Massachusetts
Institute of Technology, Cambridge,
MA, USA
Qian Chen
Department of Mechanical
Engineering, Massachusetts
Institute of Technology, Cambridge,
MA, USAAngela Dai
Department of Computer Science,
Technical University of Munich,
Munich, GermanyFaez Ahmed
Department of Mechanical
Engineering, Massachusetts
Institute of Technology, Cambridge,
MA, USA
ABSTRACT
We introduce the concept of â€œDesign Agentsâ€ for engineer-
ing applications, particularly focusing on the automotive design
process, while emphasizing that our approach can be readily ex-
tended to other engineering and design domains. Our framework
integrates AI-driven design agents into the traditional engineer-
ingworkflow,demonstratinghowthesespecializedcomputational
agents interact seamlessly with engineers and designers to aug-
ment creativity, enhance efficiency, and significantly accelerate
the overall design cycle. By automating and streamlining tasks
traditionally performed manually, such as conceptual sketching,
styling enhancements, 3D shape retrieval and generative mod-
eling, computational fluid dynamics (CFD) meshing, and aero-
dynamic simulations, our approach reduces certain aspects of
the conventional workflow from weeks and days down to min-
utes. Theseagentsleveragestate-of-the-artvision-languagemod-
els (VLMs), large language models (LLMs), and geometric deep
learningtechniques,providingrapiditerationandcomprehensive
design exploration capabilities. We ground our methodology in
industry-standard benchmarks, encompassing a wide variety of
conventionalautomotivedesigns,andutilizehigh-fidelityaerody-
namic simulations to ensure practical and applicable outcomes.
Furthermore, we present design agents that can swiftly and ac-
curatelypredictsimulationoutcomes,empoweringengineersand
designers to engage in more informed design optimization and
exploration. This research underscores the transformative poten-
tialofintegratingadvancedgenerativeAItechniquesintocomplex
engineeringtasks,pavingthewayforbroaderadoptionandinno-
vation across multiple engineering disciplines.
âˆ—Corresponding author: mohamed.elrefaie@mit.edu
Preprint - Under ReviewKeywords: AI Agents, Generative AI, DrivAerNet, Car Design
1. INTRODUCTION
The design of a car is a multidisciplinary endeavor that bal-
ances engineering performance with aesthetic appeal. Unlike
aircraft design, wherefunctional performance and safetytake ab-
solute priority, automotive design is deeply influenced by styling
and brand identity, making it both an engineering and artistic
process [1â€“9]. Consumers are drawn to cars not only for their
technicalspecificationsbutalsofortheirvisualappeal,asaesthet-
ics play a crucial role in market success [10]. Consequently, car
designinvolvesacomplexinterplaybetweenaerodynamics,man-
ufacturability,andsubjectiveuserpreferences. Traditionaldesign
workflows rely on iterative refinements where designers sketch
conceptual ideas, evaluate existing designs for inspiration, and
collaborate with engineers to assess performance and feasibility.
While traditional car design workflows often involve a slow
and iterative process of sketching, evaluation, and engineering
refinement, we propose a novel approach that integrates AI De-
sign Agents into the conceptual design phase. In this context,
agents are specialized, autonomous computational systems de-
signed to perform specific design tasks by leveraging artificial
intelligence, machine learning, and automation. These agents
automate critical tasks, streamlining the transition from initial
sketches to fully simulated aerodynamic evaluations. The multi-
agent framework introduced in this work covers key stages of
the car design pipeline, beginning with early-stage sketching and
styling, incorporating large language models (LLMs) and vision-
language models (VLMs) such as Stable Diffusion XL [11] and
ControlNet [12] to enhance and refine conceptual designs. The
process continues with automated 3D shape retrieval and genera-
tion,wheregeometricdeeplearningmodelstransformhand-drawn
1arXiv:2503.23315v1 [cs.AI] 30 Mar 2025
Figure 1: AIDesignAgentsforAcceleratingtheCarDesignProcess. Theframeworkintegratesvision-languagemodels(SDXL,ControlNet,DALL Â·E),
geometric deep learning models (DeepSDF, PointNet, RegDGCNN, TripNet), and LLMs to automate design tasks. AutoGen helps different agents
communicatewitheachother,improvingcoordinationandmakingthedesignprocessfasterandmoreefficient. Theagentscanalsointeractwithvarious
engineering tools and execute Python commands, enabling automation of complex design and simulation workflows.
sketches into 3D car meshes or retrieve similar designs from the
DrivAerNet++ 1[13, 14] database. Additionally, deep learning-
based surrogate models allow for rapid aerodynamic evaluations,
replacing expensive and time-consuming CFD simulations with
predictive models trained on high-fidelity data. To ensure seam-
less integration into simulation workflows, an AI-driven meshing
agentisemployedtomodifyandadaptmeshingparameterswithin
OpenFOAM [15], facilitating automated and optimized aerody-
namic analyses.
This multi-agent framework not only accelerates the design
cycle but also fosters a more collaborative interaction between
designers and engineers. By leveraging geometric deep learn-
ing and nonlinear dimensionality reduction techniques, such as
t-distributedstochasticneighborembedding (t-SNE)[16],wecan
haveabetterunderstandingofthehigh-dimensionaldesignspace
of DrivAerNet++. This approach enables the identification of
clusters of high-performance designs, facilitates the analysis of
critical design features, and provides a method for visually val-
idating CAD/CAE results. Furthermore, the entire process is
automated through Python APIs, utilizing AutoGen [17] for AI
agents orchestration.
Figure1illustratesourAI-drivenframeworkforaccelerating
the car design process, highlighting the iterative nature of collab-
oration between designers, stylists, and engineers. The conven-
tionalcardesignworkflowbeginswitha2Dsketchorconceptual
design,followedbyCADmodelingand3Dgeometrygeneration.
Subsequentstepsinvolverunningphysics-basedsimulations,such
1https://github.com/Mohamedelrefaie/DrivAerNetascomputationalfluiddynamics(CFD)andfiniteelementanalysis
(FEA),beforerefiningthedesignthroughmorphingandoptimiza-
tion. The validated designs then proceed to wind tunnel testing
and, ultimately, manufacturing.
On the other hand, our AI Design Agent framework inte-
gratesvision-languagemodels(VLMs)andlargelanguagemodels
(LLMs)alongsidegeometricdeeplearningmethods,significantly
accelerating and enhancing these traditional workflows. This in-
tegration enables seamless interaction with engineering software
such as Blender [18], OpenFOAM [15], and ParaView [19]. The
system consists of four specialized design agents:
â€¢StylingAgent ,responsibleforgeneratinghigh-resolutionren-
derings and enhancing aesthetic appeal;
â€¢CAD Agent , which facilitates design retrieval from the Dri-
vAerNet++ dataset and enables generative 3D shape model-
ing based on industry-standard designs;
â€¢MeshingAgent ,whichinteractswithOpenFOAMtogenerate
high-quality CFD meshes for aerodynamic simulations; and
â€¢Simulation Agent , which leverages surrogate models to pro-
videreal-timeaerodynamicpredictionswhilealsoretrieving
simulation results from the database.
Our approach significantly accelerates the iterative design
process,bridgingthegapbetweenconceptualization,performance
evaluation, and optimization. Figure 2 illustrates how engineers
and designers interact with the multi-agent system, integrating
conceptual sketches, styling renderings, CAD retrievals, mesh
Figure 2: Ourmulti-agentsystemenableseffectiveinteractionbetweenengineersanddesignersthroughoutthecardesignprocess. Givenaninput2D
sketch for conceptual design, the Styling Agent renders high-resolution images, enhancing visual aesthetics. The CAD Agent generates new designs
via generative modeling or retrieves 3D meshes from the DrivAerNet++ database. The Meshing Agent creates high-quality computational meshes
for CFD simulations and evaluates mesh quality. Finally, the Simulation Agent performs real-time aerodynamic performance predictions or retrieves
aerodynamicdatafromtheDrivAerNet++database,acceleratingtheiterativedesignprocess. Differentoutputsfromthevariousagentscanbeusedfor
design exploration or design optimization, enabling a data-driven and efficient approach to automotive design.
generation,andreal-timeaerodynamicsimulationsforstreamlined
automotive design.
This paper is organized as follows: Section 2 reviews re-
lated work on the integration of generative AI tools in engineer-
ing design, with a particular focus on applications in car design.
Section 3 presents the proposed methodology, detailing the im-
plementation of various AI design agents and their roles in the
conceptual design process. Section 4 provides results and ob-
servations from our experiments, highlighting the efficiency and
effectiveness of the proposed AI-driven framework. Section 5
discusses the implications of these findings for design processes
and workflows. Finally, Section 6 addresses limitations, outlines
potential avenues for future work, and concludes with reflections
on integrating generative AI into engineering applications.
2. RELATED WORK
We review existing research and industry trends related to
generative AI applications in automotive design. This provides
contextforunderstandinghowourframeworkaddressesgapsand
extends prior work.
The integration of artificial intelligence (AI) into automo-
tivedesignprocesseshasgarneredsignificantattention,providing
innovative solutions for enhancing creativity, performance eval-
uation, and overall design efficiency. Liu et al. [20] introduced
a human-centered generative design framework, integrating hu-
manfactorsandmechanicalattributesearlyinthedesignprocess.
Initial case studies suggested that this multidisciplinary approach
yieldedmorediverseandcreativeoutcomes,demonstratingthepo-
tential advantages of synthesizing human-centric considerations
within generative design workflows. Similarly, Yuan et al. [21]
explored AI-driven generative methods for vehicle front-face de-
sign, integrating tools such as ChatGPT, Midjourney, and Vega
AI. Their methodology emphasized a smooth transition through
design phases by systematically blending aesthetic and engineer-
ing criteria. Additionally, they applied Kansei engineering and
AI-driven form generation to tailor designs specifically to female
aesthetics, effectively quantifying subjective preferences and en-
hancingtheworkflowwithtargetedsolutionsalignedwithmarketdemographics.
Ananthanetal.[22]introducedamethodologythatleveraged
machine learning techniques specifically tailored for road vehicle
aerodynamics. Their approach enabled designers to rapidly it-
erate between conceptual designs and aerodynamic performance
assessmentsbyoptimizingshapeswithinalearnedlatentspace,ef-
fectivelyconnectingearly-stagedesigntoaerodynamicoutcomes.
Similarly, ArÃ©chiga et al. [23] proposed drag-guided diffusion
models for vehicle image generation, further emphasizing the in-
tegrationofaestheticsandaerodynamicconsiderationsinautomo-
tivedesignworkflows. Addressingthebalancebetweenrapidpro-
totyping and visual fidelity, Radhakrishnan et al. [24] developed
a generative design system that leveraged Generative Adversarial
Networks(GANs). Theiruniqueconvolutionalarchitecturemim-
ickeddesignersâ€™sketchingstyleduringinitialprototypingphases,
efficientlyproducinghigh-quality,novelcardesignsfromminimal
studio sketches, significantly accelerating the conceptual design
cycle.
Further enriching design capabilities, Edwards et al. [25]
presented Sketch2Prototype, a generative AI-based framework
that enabled rapid exploration and prototyping in early design
phases. This approach highlighted the potential of generative
models, specifically in streamlining the iterative process between
conceptcreationanddetailedevaluation. InRef.[26],theauthors
introduced a multi-agent system (MAS) that leveraged vision-
language models (VLMs) and specialized agents mimicking dis-
tinct engineering roles to collaboratively generate CAD models.
Their approach outperformed a single-shot generative baseline,
demonstrating enhanced readiness and requirement compliance
due to its iterative self-feedback architecture.
However,existingstudiesprimarilyfacelimitationsduetothe
lack of large-scale, multimodal datasets that combine diverse de-
sign variations with high-fidelity simulations. This research gap
has constrained the effective coupling of aesthetics and design
performance evaluation, limiting the ability to optimize designs
holistically. Our work addresses this challenge through the Dri-
vAerNet++ dataset and the introduction of Design Agents . By
Designer Prompt
We are in the conceptual design phase of an estateback car and I want to explore different styling options. I have provided a
sketch of the initial design, and I would like to generate high-quality renderings based on specific styling directions. Could you
assist in creating variations of the design using the following prompts?
â€¢A futuristic sports car, aggressive design aerodynamic, vibrant red color.
â€¢A classic vintage car pearlescent champagne color, chrome details, and a
luxurious 1920s style.
â€¢Aruggedoff-roadSUV,allterrainsvehicle,matteblack,robustandmuscular
design.
AutoGen - Orchestrating AI Design Agents
Activating the Styling Agent to generate high-quality renderings based on the specified design prompts.
Styling Agent Response
High-resolution renderings have been gener-
ated, ensuring accurate representation of the
specified aesthetics.
Engineer Prompt
Basedonthegeneratedrenderings,couldyoupleasesearchtheDrivAerNet++databaseforsimilardesigns? Ifyoufindrelevant
matches,pleasealsoretrievethecorrespondingCFDsimulationdata. Thiswillhelpinevaluatingaerodynamicperformanceand
guiding further refinements.
AutoGen - Orchestrating AI Design Agents
Calling the CAD Agent to retrieve similar 3D designs from DrivAerNet++ and the Simulation Agent to provide aerodynamic
analysis, including CFD simulation data for performance evaluation.
CAD Agent Response
Retrievedthetop3designssimilartotheestatebacksketch
from the DrivAerNet++ database with their IDs and STL
files. A Blender window will open with the STL files
loaded,allowingfurtherinspectionandmodificationofthe
retrieved designs.
Simulation Agent Response
Aerodynamic simulation retrieval has been completed for
the three designs. Detailed reports have been generated,
includingvisualizationsofvelocitydistributioninthesym-
metry plane to analyze flow characteristics. A Paraview
windowwillopenwiththeVTKfilesloaded,allowingfur-
ther inspection and modification of the retrieved designs.
Figure 3: Example use case of the AI-driven multi-agent system for car design. Designers start with a 2D sketch, which the Styling Agent uses to
generate high-resolution renderings. The CAD Agent then retrieves similar 3D designs from DrivAerNet++ for further exploration. Engineers can
request aerodynamic analysis, and the Simulation Agent provides CFD simulation data, allowing for rapid performance evaluation and iterative design
improvements.
Designer Prompt
I would like to explore intermediate designs between two car configurations. Could you interpolate between the 3D mesh of the
estateback and the notchback car, both with smooth underbodies? The goal is to generate a smooth transition between these two
designs while preserving aerodynamic integrity.
AutoGen - Orchestrating AI Design Agents
Calling the CAD Agent.
CAD Agent Response
Three intermediate shapes have been gen-
erated by interpolating in the latent space.
These interpolations smoothly transition be-
tween the estateback and notchback de-
signs while maintaining geometric consis-
tency. The generated shapes are now avail-
able for further evaluation and aerodynamic
analysis.
Engineer Prompt
Since the interpolated shapes are newly generated and not part of the existing dataset, I would like to run a CFD simulation to
analyze their aerodynamic performance. Could you please generate a high-quality computational mesh for the first interpolated
design using SnappyHexMesh?
AutoGen - Orchestrating AI Design Agents
Calling the Meshing Agent.
Meshing Agent Response
GeneratedCFDmeshforthespecifieddesignswithrefinementsincriticalareas. Thecomputationaldomainwasfirstinitialized
usingblockMeshDict ,followedbymeshrefinementandsurfacesnappingwith snappyHexMesh . Finally, checkMesh wasexecuted
to verify mesh quality, ensuring all key metrics are within acceptable limits. Log files for all steps have been saved.
Figure 4: Continuation of the AI-driven multi-agent system use case for car design. After retrieving similar designs, the CAD Agent generates
intermediate shapes through latent space interpolation. These interpolated designs transition smoothly between the estateback and notchback configu-
rationswhilemaintaininggeometricconsistency. Sincethesenewshapesarenotpartoftheexistingdataset,theMeshingAgentgenerateshigh-quality
computational meshes for aerodynamic analysis. Engineers can then run CFD simulations to evaluate performance, enabling iterative refinement and
optimization.
integrating automation, collaboration, and intelligent decision-
making, these agents enable rapid design iteration. Traditional
car design workflows, from conceptual sketching to simulation,
can take several weeks, whereas our multi-agent framework re-
duces this timeline to minutes by automating 3D shape retrieval,
CFD meshing, and aerodynamic evaluation. The Design Agents
operate on a large-scale, high-fidelity CFD dataset, distinguish-
ing our approach by utilizing an extensive collection of 8,000
industry-standard car designs and addressing both aerodynamic
performance and aesthetic considerations. Figures 3 and 4 illus-
tratearepresentativeusecasefromourstudy,demonstratinghow
engineersanddesignerscaneffectivelyinteractwithourproposedmulti-agent design system.
3. METHODOLOGY: A MULTI-AGENT AI FRAMEWORK
FOR CAR DESIGN
Thissectionpresentsourmulti-agentAIframework,explain-
ing how various specialized AI agents collaboratively automate
and streamline the entire automotive design process. By seam-
lessly integrating large language models (LLMs), vision-based
latent models such as Stable Diffusion XL [11] with Control-
Net [12], and geometric deep learning models (DeepSDF [27],
PointNet [28], RegDGCNN [13, 14, 29], and TripNet [30]) us-
ing the AutoGen framework [17], we demonstrate a collaborative
Figure 5: Different data representations and modalities from DrivAerNet++, a dataset comprising 8,000 industry-standard car designs. These
modalitiesâ€”including 3D CAD models, 3D meshes, point clouds, voxel grids, depth maps, and part annotationsâ€”are leveraged by various generative
AI models depending on the design task, such as retrieval, 3D reconstruction, styling, and aerodynamic simulation. In this work, we further extend
DrivAerNet++byaddingSignedDistanceFunction(SDF)representations,multi-viewimages,andsketchestosupportdiversegenerativedesigntasks.
multi-agent system capable of tackling various design tasks.
Our multi-agent system facilitates collaboration between en-
gineers and designers in the car design process. Given a 2D
conceptualsketch ,theStylingAgent ,poweredbySDXL[11]and
ControlNet [12], generates high-resolution renderings with text-
drivenstyling. The CADAgent retrievessimilar3Ddesignsfrom
DrivAerNet++orgeneratesnewshapes. The MeshingAgent pro-
duces CFD-ready meshes and evaluates their quality. Finally, the
Simulation Agent predicts aerodynamic properties in real time
orretrievesCFDdatafromDrivAerNet++,enablingrapiddesign
evaluation and optimization.
To orchestrate and coordinate interactions among our pro-
posed AI Design Agents, we leverage AutoGen [17], a frame-
work designed for efficient multi-agent collaboration. Within
this approach, we specifically examine two essential con-
ceptsâ€”Sequential vs. Hierarchical vs. Hybrid Agents andCross-
Modal Retrieval â€”as they critically influence the effectiveness of
our agent-based automotive design workflow.
AutoGen: A Framework for Building AI Agents and Appli-
cations In our work, we leverage AutoGen [17], a framework
developed by Microsoft for creating multi-agent AI applications
that can act autonomously or work alongside humans. It allows
agents to effectively exchange information through natural lan-
guage prompts, enabling the automation of complex workflows
involving diverse software platforms. In our study, Autogen co-
ordinates interactions between specialized design agents, such as
CAD retrieval, mesh generation, aerodynamic simulation, and
styling agents, streamlining the engineering design process and
enhancing productivity and creativity. The integration provided
by AutoGen significantly accelerates tasks by reducing manual
intervention, facilitating rapid design iterations, and improving
collaborative decision-making.Sequential vs. Hierarchical vs. Hybrid Agents Sequential,
hierarchical, and hybrid agents represent different structures for
AI-drivenworkflows[31]. Sequentialagentsoperateinastep-by-
stepmanner,whereoneagentcompletesataskbeforepassingthe
outputtothenext(e.g.,asketchingagentgeneratesa2Dconcept,
which is then refined by a styling agent, followed by a simu-
lation agent for aerodynamic evaluation). Hierarchical agents,
on theother hand,follow astructured decision-makingapproach,
whereacentralagent(orhigher-levelcontroller)delegatestasksto
specializedsub-agents,ensuringcoordinationandgoalalignment.
Thisapproachisbeneficialwhentasksrequirecomplexdependen-
cies and prioritization. Hybrid agents combine both approaches,
allowing agents to work both sequentially and in parallel, where
certaintasksmayrunindependentlywhileothersfollowadefined
workflow.
Incardesign,thehybridagentapproachisthemostrelevant,
astheprocessinvolvesbothsequentialdependencies(e.g.,starting
with sketches, then generating 3D shapes, and finally evaluating
aerodynamics) and parallel tasks (e.g., styling and aerodynamic
simulation can be iterated independently before final validation).
Thisflexibilityallowsengineersanddesignerstooptimizeperfor-
mancewhilemaintainingcreativefreedominshapingcaraesthet-
ics.
Cross-Modal Retrieval Cross-modal retrieval in DrivAer-
Net++ enables engineers and designers to access aerodynamic
performancedataandparametricdesigninformationusingdiverse
input modalities. By leveraging deep learning-based retrieval
models, a simple 2D hand-drawn sketch of a car can be used to
retrieve the most aerodynamically similar 3D designs from the
dataset,alongwiththeircorrespondinghigh-fidelityCFDsimula-
tions. Thisallowsdesignerstoexploreoptimizedshapesthatbal-
ance aesthetics and performance, while engineers can efficiently
search for design variations with similar aerodynamic properties.
Theintegrationofimage-andtext-basedquerieswithengineering
data enhances the design workflow, bridging conceptual sketches
with performance-driven design evaluation. Figure 5 illustrates
the diverse data modalities in DrivAerNet++, including 3D CAD
models, meshes, point clouds, voxel grids, depth maps, and part
annotations, while also highlighting our extensions with Signed
DistanceFunction(SDF)representations,multi-viewimages,and
sketches to enhance generative design tasks.
3.1 Enhancing Aesthetic Exploration with the Styling
Agent
Inthissection,weintroducethe StylingAgent todemonstrate
an agentic way of providing design inspirations in automotive
design workflows. Given a hand-drawn sketch accompanied by
a text prompt, our Styling Agent leverages advanced generative
AI models to produce high-resolution, photorealistic renderings
ofcardesigns. Thisenablesdesignerstorapidlyvisualizediverse
aesthetic concepts that are inspired by their sketches and iterate
creatively in early design phases.
To demonstrate our Styling Agent, we required a dataset of
car sketches. As no public dataset exists, we created our own
datasetbasedontheDrivAerNet++dataset. Curatingadatasetof
8,000hand-drawnsketchesthatcorrespondtocarsintheDrivAer-
Net++ dataset is challenging due to the labor-intensive nature of
manual sketching and the variability in artistic styles. Therefore,
weemploytwoautomatedapproachestoovercomethischallenge:
a traditional computer vision method using Canny edge detec-
tion [32] and a pre-trained generative AI approach leveraging
CLIPasso model [33] to produce sketch abstractions from input
images. Prior work has shown that the output of AI generated
sketchesalignswellwithhuman-drawnsketches. Unlikemethods
requiringexplicitsketchdatasetsfortraining,ourapproachgener-
ates sketches 2that closely resemble those drawn by designers in
early-stage conceptual design, ensuring consistency and scalabil-
ity in capturing design intent.
(a)Generated with 1
sketch, 25 strokes.
(b)Generated with 1
sketch, 50 strokes.
(c)Generated with 1
sketch, 100 strokes.
(d)Generated with 3
sketches, 25 strokes.
(e)Generated with 3
sketches, 50 strokes.
(f)Generated with 3
sketches, 100 strokes.
Figure 6: Sketches of an Estateback car from the DrivAerNet++
dataset[14],generatedusingCLIPasso[33]. Thesketchesvarybasedon
thenumberofstrokesandinputsketchesused,demonstratingthemodelâ€™s
ability to capture design features with different levels of abstraction.
2CLIPasso and Canny edge detection are used solely for generating a dataset of
hand-drawn sketches to train and evaluate shape and data retrieval models. The
agent itself does not utilize these methods during operation. Instead, it performs
retrievalbasedonuser-providedsketches,undertheassumptionthathuman-drawn
sketches resemble those generated by CLIPasso or Canny edge detection.Sketch Generation Using CLIPasso This subsection shows
how we leverage the CLIPasso model [33] to generate expres-
sive and representative sketch abstractions from realistic car im-
ages, facilitating early-stage design exploration. The CLIPasso
modelisaframeworkforgeneratingsketchesusingdifferentiable
vector graphics guided by neural optimization [33]. It leverages
CLIP (Contrastive Language-Image Pre-training) [34], a vision-
language model, to iteratively refine sketches based on textual
descriptions or visual concepts. By optimizing BÃ©zier curves di-
rectlyinadifferentiablemanner,CLIPassoefficientlysynthesizes
minimalisticyetexpressivesketchesthatcloselymatchthedesired
semantic content [33]. This approach bridges neural rendering
techniques and vector graphics, providing a precise and intuitive
method for sketch-based conceptual design tasks and interactive
creativity support [33, 35].
Figure6presentssketchesofanEstatebackcarfromtheDri-
vAerNet++dataset,showcasingvariationsbasedonthenumberof
inputsketchesandthenumberofstrokesusedforabstraction. The
toprowdepictssketchesgeneratedwithasinglesketch,whilethe
bottomrowshowsthosegeneratedusingthreesketches,witheach
column corresponding to an increasing number of strokes. Here,
increasingthenumberofstrokesenhancesthequalityandlevelof
detail in the sketch, allowing for a more precise representation of
design features.
ControlNetforGuidedConceptualDesignGeneration Con-
trolNet [12] is a conditional image generation architecture built
upon diffusion-based generative models. It incorporates external
guidancesignalsâ€”suchasedgedetectionmaps,semanticsegmen-
tations, or user-generated sketchesâ€”to precisely control the gen-
erative process. By embedding these explicit control signals into
diffusion models, ControlNet enables designers to transform pre-
liminary sketches into refined, high-resolution renderings while
preserving original stylistic and structural intent. In the context
ofautomotivedesign,ControlNetâ€™scontrolledgenerationprocess
facilitatesaccurateandrapidconceptualexploration,significantly
streamliningiterativedesignrefinementandenhancingalignment
between stylistic concepts and engineering requirements.
High-resolutionandPhotorealisticRenderingsofCarDesigns
This subsection introduces the process of using vision-language
models, such as Stable Diffusion XL (SDXL) [11] and Con-
trolNet [12], to produce photorealistic and stylistically diverse
car renderings conditioned on user sketches and text prompts.
After generating the sketches using either Canny edge detec-
tion [32] and CLIPasso [33] for abstraction-based sketch gen-
eration, these sketches serve as conditioning inputs for a Con-
trolNet model [12], which refines text-driven generation using
Stable Diffusion v1.5 [11, 36]. We use a combination of base
car design prompts, color variations, and stylistic attributes to
introduce diverse and creative designs. The generated images
are then post-processed to remove the background using mor-
phological operations and contour-based segmentation. The final
outputs, along with metadata including the applied prompts, are
stored for further analysis. This approach can allow designers
toexploremultipleaestheticvariationswhilemaintainingtheun-
derlying structural integrity of the design. To introduce stylistic
Figure 7: Input and output of the Styling Agent . The input consists of a hand-drawn sketch and a text prompt, which guides the model to generate
diverse car renderings. The results demonstrate variations in styling, materials, and lighting based on different text prompts. Two car categories are
shown: sedan (fastback) in the top row and SUV (estateback) in the bottom row, highlighting how AI-driven design can adapt to user preferences and
generate high-quality, photorealistic concepts.
diversity, we experiment with different car categories and artistic
influences by modifying design prompts. Table 1 summarizes
the various styling and color variations applied to generate a di-
verse dataset of rendered car designs. By leveraging AI-driven
design agents, we synthesized 8,000 rendered car designs, each
incorporating different stylistic influences and color schemes to
enhance visual diversity. This approach allows for a broad explo-
ration of aesthetic variations, ensuring that the generated dataset
captures a wide spectrum of design possibilities across multiple
car categories.
Category Style Applied Color Variation
Futuristic Sports Car Cyberpunk, High-Tech Metallic Silver, Neon Blue
Classic Vintage Car Steampunk, Luxury Deep Red, Matte Gold
Off-Road Vehicle Rugged, Urban Earthy Brown, Matte Gray
Electric Concept Car Minimalist, Futuristic Pearl White, Electric Green
Race Car Aerodynamic, Sporty Glossy Black, Sunset Orange
City Car Urban, Eco-Friendly Midnight Purple, Bright Yellow
Retro-Inspired Car Vintage, Artistic Vibrant Blue, Pastel Pink
Table 1: Style and color variations used for enhanced diversity.
We show in Figure 7 the transformation of hand-drawn
sketches into high-quality car renderings using the Styling Agent .
The input consists of a 2D sketch obtained from either Canny or
CLIPasso, combined with text prompts that guide the generation
process. Theresultsshowcasediversestylingvariations,material
finishes, and lighting effects, adapting to different design intents.
Two car categories are depicted: fastback sedans (top row) and
estatebackSUVs(bottomrow),illustratinghowAI-drivendesign
enablesrapidexplorationofaestheticconceptswhilemaintaining
structural consistency.
3.2 Bridging 2D Sketches and 3D Geometries with the
CAD Agent
In this section, we introduce the CAD Agent 3, which bridges
conceptual sketches and detailed 3D car designs. Given a 2D
hand-drawn sketch, the CAD Agent searches the DrivAerNet++
database in real-time to retrieve the most similar existing 3D de-
sign. Beyondretrieval,itcanperformgenerativetasks,synthesiz-
3To avoid confusion, we differentiate it from the Meshing Agent, which focuses
exclusively on generating computational meshes for CFD simulations.ing novel 3D designs and interpolating between two input mod-
els to explore intermediate shapes, all conditioned on industry-
standardautomotivegeometries. Thisfacilitatesrapidexploration
and generation of feasible, production-oriented concepts.
3.2.1 Deep Implicit Representations for Automotive
Shapes. Below we outline our approach using DeepSDF to
encode automotive shapes into continuous implicit representa-
tions, enabling efficient shape retrieval and generative modeling.
DeepSDF [27] is a neural implicit representation technique that
encodes 3D shapes into a continuous Signed Distance Function
(SDF) space, allowing for efficient shape reconstruction and in-
terpolation. Our approach extends DeepSDF for car design by
encoding car meshes into a latent space representation while in-
troducingmodificationstoimprovetrainingefficiencyandrecon-
structionquality. RecentresearchhasleveragedDeepSDFfor3D
car generation and customization. Morita et al. [37] introduced
VehicleSDF, enabling the generation of diverse 3D car models
while estimating aerodynamic performance via surrogate model-
ing. Similarly,Miaoetal.[38]proposedaframeworkforprecise,
attribute-specificmodificationstobothstylisticandgeometricas-
pects of car designs, ensuring structural integrity.
DeepSDFTrainingData,NetworkArchitectureandModifica-
tionsTotrainourCADAgent,werequireadatasetthatincludes
SDFsampleswhichareessentialfortrainingimplicitneuralshape
representations. To generatethe training data, wenormalizeeach
car mesh to fit within a unit sphere and sample 500,000 spatial
points, with a higher concentration near the surface. Each point
(ğ‘¥,ğ‘¦,ğ‘§)isassociatedwithanSDFvaluethatrepresentsthesigned
distance to the nearest surface.
DeepSDF uses an auto-decoder framework, where a latent
vector zâˆˆRğ‘‘is learned for each training shape. The network
consists of a multi-layer perceptron (MLP) with eight fully con-
nected layers of 512 neurons each, utilizing ReLU activations.
Given an input coordinate xâˆˆR3and the latent vector represen-
tation zâˆˆRğ‘‘, the model predicts the corresponding SDF value
ğ‘“(x,z).
To improve training efficiency, accuracy, and performance
specific to car designs, we introduce the following modifications:
Figure 8: Retrieval success rate of our CAD agent for different car categories in the DrivAerNet++ dataset. The ğ‘¥-axis represents the percentile rank
of the correctly retrieved latent code within the search space, while the ğ‘¦-axis shows the frequency of retrieval attempts. Each subplot corresponds to
a different car configuration, with the success rate indicating the percentage of cases where the correct latent code was ranked among the top 1% of
retrievedcandidates. Theresultsdemonstratetheeffectivenessofourretrievalmodelinaccuratelyidentifyingtheclosestmatching3Dcarmeshfroma
given input sketch.
â€¢Positional Encoding: Inspired by recent advancements in
neural implicit representations [38], we apply a Fourier fea-
ture mapping to input coordinates:
ğ›¾(x)=(ï¸‚
sin(2ğ‘˜ğœ‹x),cos(2ğ‘˜ğœ‹x))ï¸‚
, ğ‘˜=0,..., 8(1)
Thistransformationaidsinlearninghigh-frequencygeomet-
ric details.
â€¢Reduced Latent Space Dimension: The latent vector di-
mension is reduced from 256 to 16 to optimize storage and
improve generalization in the constrained design space of
automotive shapes.
TrainingProcedureandReconstruction Themodelistrained
on SDF samples using L1 loss on predicted SDF values, with L2
regularization applied to the latent codes. During training, both
the model parameters and latent codes are optimized simultane-
ously using the Adam optimizer. The loss function is defined
as
L(ğ‘“ğœƒ(x),ğ‘ )=|clamp(ğ‘“ğœƒ(x),Â±ğ›¿)âˆ’clamp(ğ‘ ,Â±ğ›¿)|(2)
where clamp(ğ‘¥,ğ›¿):=min(ğ›¿,max(âˆ’ğ›¿,ğ‘¥))ensures that the
model focuses its representational ability near the surface. The
total loss for optimizing the model and latent codes is:
arg min
ğœƒ,{ğ‘§ğ‘–}ğ‘
ğ‘–=1ğ‘âˆ‘ï¸‚
ğ‘–=1â›âœ
âğ¾âˆ‘ï¸‚
ğ‘—=1L(ğ‘“ğœƒ(ğ‘§ğ‘–,ğ›¾(xğ‘—)),ğ‘ ğ‘—)+ğœ†âˆ¥ğ‘§ğ‘–âˆ¥2
2ââŸ
â (3)Givenatargetshape,wefindtheassociatedlatentcodebyus-
ingtheAdamoptimizertominimizethereconstructionloss. Then,
wequerytheDeepSDFmodelatspatialpoints (ğ‘¥,ğ‘¦,ğ‘§)andextract
the final mesh using the Marching Cubes algorithm [39]. To ex-
plore the learned latent space, we perform interpolation between
latent representations of different car designs, enabling smooth
transitions between shape variations.
3.2.2 Retrieval of 3D Car Models from 2D Sketches.
Here we demonstrate how the CAD Agent retrieves the most
similar 3D car geometries from DrivAerNet++ based on input
sketches.
Latent-Code-Based Shape Retrieval This subsection de-
scribes how latent space representations (from DeepSDF) enable
efficientidentificationofmatchingcargeometriesbasedonsketch
inputs. Our approach involves training a Convolutional Neural
Network (CNN) to predict the latent code associated with each
car mesh based on the input sketch. The CNN takes a sketch as
inputandoutputsthepredictedDeepSDFlatentcode. Themodel
is trained to minimize the L2 loss between the predicted latent
code and the ground truth latent code. We use a single CNN
model for all car classes. During retrieval, the input consists of
a car class and a sketch, and the output is the car mesh with the
closest matching latent code, determined by minimizing the L2
distancebetweenthepredictedlatentcodeandthelatentcodesof
meshes within the specified car class:
Ë†ğ‘§=ğ‘“ğœƒ(sketch,ğ‘) (4)
mesh=arg min
meshâˆˆMğ‘âˆ¥ğ‘§meshâˆ’Ë†ğ‘§âˆ¥2 (5)
where:
â€¢ğ‘“ğœƒisaCNNtrainedtopredicttheDeepSDFlatentcodefrom
a given sketch.
â€¢ğ‘is the specified car class.
â€¢Ë†ğ‘§is the predicted latent code of the input sketch.
â€¢ğ‘§meshrepresentsthe latentcodesof available3Dcar meshes.
â€¢Mğ‘is the set of all meshes belonging to the car class ğ‘.
â€¢TheretrievalprocessselectsthemeshwhoseDeepSDFlatent
code minimizes the L2 distance to the predicted latent code
Ë†ğ‘§.
The retrieval success rates for different car configurations
within the DrivAerNet++ dataset are depicted in Figure 8. The
ğ‘¥-axis represents the percentile rank of the correctly retrieved
latentcodewithinthesearchspace,whilethe ğ‘¦-axisindicatesthe
frequency of retrieval attempts. Each subplot corresponds to a
specific car configuration, demonstrating the effectiveness of our
retrieval model in accurately identifying the closest matching 3D
car mesh from a given input.
Feature-BasedRetrievalviaCosineSimilarity Below,wedis-
cussouralternativeapproachforshaperetrieval,usingcosinesim-
ilaritymetricsonCNN-extractedimagefeaturestorobustlyiden-
tify similar car designs. The similarity between a query image
and a set of database images is computed using cosine similar-
ityinahigh-dimensionalfeaturespacederivedfromapre-trained
ResNet-50[40]convolutionalneuralnetwork. Thefinalfullycon-
nectedclassificationlayerisremoved,yieldingafeatureextraction
modelF:Rğ»Ã—ğ‘ŠÃ—ğ¶â†’Rğ‘›, whereğ»,ğ‘Š, andğ¶represent the
height,width,andnumberofchannelsoftheinputimage,respec-
tively, andğ‘›is the dimensionality of the resulting feature vector.
Letthequeryimagebedenotedas Iğ‘anditscorrespondingfeature
representation as q=F(Iğ‘). Similarly, let the ğ‘–ğ‘¡â„image in the
database, Iğ‘–, be represented by its feature vector dğ‘–=F(Iğ‘–).
The cosine similarity score ğ‘†(q,dğ‘–)is computed as follows:
ğ‘†(q,dğ‘–)=âˆ‘ï¸ğ‘›
ğ‘—=1ğ‘ğ‘—ğ‘‘ğ‘–,ğ‘—
âˆšï¸‚âˆ‘ï¸ğ‘›
ğ‘—=1ğ‘2
ğ‘—âˆšï¸‚âˆ‘ï¸ğ‘›
ğ‘—=1ğ‘‘2
ğ‘–,ğ‘—
whereğ‘ğ‘—andğ‘‘ğ‘–,ğ‘—represent the ğ‘—ğ‘¡â„component of the query
anddatabasefeaturevectors,respectively. Thissimilaritymeasure
captures the cosine of the angle between the two vectors in the
ğ‘›-dimensionalfeaturespace,ensuringinvariancetovectormagni-
tudes. Thesimilarityscoresareaggregatedintoavector SâˆˆRğ‘€,
whereğ‘€denotes the number of images in the database.
To ensure a meaningful retrieval process, we apply two se-
quential filtering criteria: (1) rank the similarity scores in de-
scending order such that ğ‘†(q,dğ‘–1)â‰¥ğ‘†(q,dğ‘–2)â‰¥Â·Â·Â·â‰¥ğ‘†(q,dğ‘–ğ‘€),
and (2) validate the top- ğ¾ranked database images to ensure the
existence of corresponding 3D geometries (STL files). Mathe-
matically, this can be expressed as:
Rğ¾={dğ‘–ğ‘˜|ğ‘˜âˆˆ[1,ğ¾],âˆƒGğ‘–ğ‘˜âˆˆS}whereRğ¾represents the final set of ğ¾retrieved images, Gğ‘–ğ‘˜
is the 3D STL geometry associated with the ğ‘˜ğ‘¡â„ranked image,
andSdenotes the set of all valid STL files.
The resulting set Rğ¾consists of the top- ğ¾most similar
databaseimages,validatedfor3Dgeometryconsistency. Eachre-
trievalresultisaccompaniedbyitscorrespondingsimilarityscore
and STL file, enabling subsequent visualization and downstream
tasks such as shape analysis and geometric inspection.
3.2.3 Generating and Exploring New Designs in 3D La-
tent Spaces. Here, we illustrate the generative capabilities
of the CAD Agent in synthesizing novel, intermediate automo-
tive shapes by smoothly interpolating in learned latent spaces of
DeepSDF. Given the latent codes of two or more car models, a
weighted average of these codes produces a new latent represen-
tationthatblendsfeaturesoftheinputdesigns. Whendecodedby
the DeepSDFnetwork, the resultingshapes are visuallyplausible
andmaintainrealism,showcasingthelatentspaceâ€™sabilitytocap-
turemeaningfulgeometricrelationshipsandsupportnoveldesign
generation.
Figure 9 demonstrates the effectiveness of our modified
DeepSDF in generating smooth transitions between various car
designs,suchasestateback,fastbackwithsmoothunderbody,fast-
backwithdetailedunderbody,andnotchbackconfigurations. This
approachallowsforconditioningontwodesignsandinterpolating
intermediate designs, facilitating a structured exploration of the
design space for aerodynamic and stylistic variations in automo-
tive applications.
Our CAD Agent leverages a learned latent space to facili-
tateefficient3Dcarshapegeneration,retrieval,andinterpolation,
enabling smooth, high-quality reconstructions essential for auto-
motivedesignworkflows. Byutilizingtheseimplicitshaperepre-
sentations,ourframeworkeffectivelygeneratesdiversegeometries
andseamlesslyinterpolatesbetweenexistingdesigns,significantly
enhancing the flexibility and creativity of AI-assisted automotive
design tasks.
3.3 Automating Computational Meshing with the
Meshing Agent
In this section, we introduce the Meshing Agent , responsible
forgeneratinghigh-qualitycomputationalmeshesfrom3Dcarde-
signs to enable accurate CFD simulations. Given a 3D car model
and text prompts specifying meshing requirements, the Meshing
Agent interacts with OpenFOAMâ€™s snappyHexMesh utility, au-
tomatically producing refined computational meshes. The agent
further verifies mesh quality through standard checks and itera-
tively improves mesh fidelity, ensuring optimal performance for
subsequent aerodynamic analyses.
Recent advancements in leveraging LLMs have demon-
strated promising results in automating and enhancing CFD sim-
ulations. For instance, Pandey et al. [41] introduced Open-
FOAMGPT, a retrieval-augmented generation (RAG) agent tai-
lored for OpenFOAM-based CFD tasks, showcasing its ability to
handle complex simulations efficiently. Similarly, Xu et al. [42]
developedanLLMagentforfiredynamicssimulations,enhancing
the usability of FireFOAM by enabling natural language interac-
tions for case configuration and simulation evaluation. Addition-
ally, Chen et al. [43] proposed MetaOpenFOAM, an LLM-based
Figure 9: Interpolation between four different car designs using our modified DeepSDF. The original input meshes (dark grey) include fastback
with detailed underbody, fastback with smooth underbody, estateback, and notchback configurations, while the interpolated meshes (light grey) are
generated by interpolating in DeepSDFâ€™s latent space. The grid showcases smooth transitions between these shape representations, demonstrating
DeepSDFâ€™s capability to generate intermediate geometries by blending latent representations. This structured exploration of the design space enables
shape optimization and generative design for aerodynamic and stylistic variations in automotive applications.
multi-agent framework designed to automate CFD simulations
usingnaturallanguageinputs,furtherstreamliningthesimulation
workflow.
SnappyHexMesh[44]isanadvancedmesh-generationutility
in OpenFOAM [15], a C++-based computational fluid dynamics
software,specificallydesignedtogeneratecomputationalmeshes
from complex geometries, widely utilized in automotive aerody-
namic simulations. It employs a robust hex-dominant meshing
technique, capable of accurately capturing intricate geometry de-
tails by refining mesh cells around surfaces and within specified
regions [44]. This method allows precise representation of com-
plex automotive geometries, enabling high-fidelity simulations.
Mesh generation typically constitutes the most time-consuming
step in CFD workflows and significantly impacts the quality and
accuracyofsimulationresults. Thus,efficientandaccuratemesh-
ing is essential for reliable aerodynamic analysis.
Interactive Mesh Generation and Refinement For the Mesh-
ingAgent,weutilizeOpenAIâ€™sAPI(version0.28)withtheGPT-
3.5-turbo model [45] to integrate LLMs with Python scripts,
automating CFD simulations using OpenFOAM. This setup en-
ables the interaction between natural language queries and engi-
neering workflows, allowing LLMs to execute OpenFOAM com-
mands, configure solver settings, and retrieve simulation results.
Additionally, SnappyHexMesh is employed for generating high-
quality CFD meshes, ensuring accurate aerodynamic analysis
while streamlining the meshing and simulation pipeline. Fig-
ure 10 illustrates an example conversation between an engineerand the Meshing Agent, refining the computational mesh for im-
proved aerodynamic accuracy. The Meshing Agent is capable
of executing various OpenFOAM utilities, such as generating the
computationaldomainwith blockMeshDict ,partitioningthemesh
usingdecomposeParDict for parallel execution, modifying and
runningsnappyHexMesh for mesh refinement, interpreting the
output of checkMesh , and visualizing the mesh quality and re-
finement regions. This integrated capability ensures efficient and
robust preparation of computational meshes, crucial for accurate
CFD simulations.
Automatic Mesh Quality Verification and Optimization Af-
ter each mesh generation, the Meshing Agent automatically exe-
cutesOpenFOAMâ€™s checkMesh utilitytoassessmeshqualityfrom
bothgeometricandtopologicalperspectives,assummarizedinTa-
ble 2. While passing these checks does not guarantee simulation
convergence,itisanessentialsteptoensuresuitablegeometryand
topology characteristics, significantly contributing to simulation
stability and accuracy.
3.4 Real-time Aerodynamic Analysis with the
Simulation Agent
TheSimulation Agent enables real-time aerodynamic perfor-
mance assessment and design retrieval using the DrivAerNet++
dataset. Given a 2D hand-drawn sketch or a 3D design, it effi-
ciently searches through the database of 8,000 high-fidelity CFD
simulations,retrievingdesignsbasedongeometricshapesimilar-
ityoraerodynamicperformancemetrics. Additionally,itleverages
surrogate models to rapidly predict aerodynamic properties such
Engineer Prompt
Generateahigh-qualitycomputationalmeshforanestatebackcarinOpenFOAM.SetupavirtualwindtunnelwithblockMeshDict,
splitthedomainacross128CPU-cores,andrunSnappyHexMeshforrefinement. Then,executecheckMeshtoverifyqualityand
plot the mesh in the y-symmetry plane.
Meshing Agent Response
ThevirtualwindtunnelhasbeensetupusingblockMeshDict,andthecompu-
tational domain has been parallelized over 128 CPU cores. SnappyHexMesh
wasexecutedwithcastellatedmeshing,snapping,andboundarylayeraddition
to refining the mesh around the car body, wheels, and underbody. A refine-
mentregionwasappliedtoimprovewakeresolution,andcheckMeshverified
that all quality metrics are within acceptable limits. The final mesh has been
visualized in the y-symmetry plane and is ready for CFD simulations. Log
files are saved.
Engineer Prompt
Thetransitionbetweenlayersaroundthecarbodyandawayfromthecariskindacoarse. Couldyoupleaseaddanextrarefinement
layer or a volume refinement region to reduce large variations in cell sizes?
Meshing Agent Response
An additional volume refinement region has been introduced to minimize
abrupt cell size variations. The total cell count increased by 17.11%, main-
taining a well-structured mesh without excessive clustering. The maximum
aspectratioandnon-orthogonalityremainwithinlimits,ensuringstabilityfor
CFDsimulations. Themeshqualityhasbeenchecked,andallmetricsremain
within acceptable limits.
Engineer Prompt
The wake region does not appear refined enough. Iâ€™m concerned that the wake flow and flow field separation might not be
captured correctly. Can we improve this?
Meshing Agent Response
Two additional volumes of refinement have been added in the wake region
to improve resolution and better capture flow separation. As a result, the
total number of cells in the third mesh increased by approximately 237.75%
compared to the first mesh and 188.40%compared to the second mesh.
Figure 10: Example conversation between an engineer and the Meshing Agent, refining the computational mesh for improved aerodynamic accuracy.
TheMeshingAgentcanrunOpenFOAMcommands,post-processresults,andanalyzemeshqualitytoensureoptimalrefinementforaccuratesimulations.
Figure 11: Illustration of design retrieval from DrivAerNet++ using either geometric shape similarity or aerodynamic performance metrics. Given
a query design, similar cars are identified based on shape or aerodynamic performance characteristics, enabling efficient exploration and optimization
of car designs. It also quantifies the improvement in terms of aerodynamic drag by showing the difference in drag coefficient ( Î”ğ¶ğ·) with respect to a
baseline design.
Quality Test Mesh 1 Mesh 2 Mesh 3
Boundary Openness " " "
Max Aspect Ratio " " "
Min Face Area " " "
Max Face Area " " "
Min Volume " " "
Max Volume " " "
Mesh Non-Orthogonality " " "
Face Pyramids " " "
Max Skewness " " "
Coupled Point Match " " "
Table 2: Meshqualitytestresultsforthreedifferentmeshes. "indicates
the test was passed, while %indicates failure.
asdragcoefficientandflowpatterns,allowingdesignerstoimme-
diatelyquantifypotentialperformanceimprovementsandanalyze
the associated flow characteristics. The results from the Simula-
tion Agent support design exploration and optimization tasks by
offeringinstantinsightsintoaerodynamicperformance,streamlin-
ing the iterative design process, and enhancing decision-making.
3.4.1 Sketch-Based Rapid Retrieval of Aerodynamic
Simulation Results. The Simulation Agent facilitates real-
time retrieval of aerodynamic performance data based on 2D
hand-drawn sketches or design inputs. Given a sketch or con-
ceptual design, it queries the DrivAerNet++ database containing
8,000 high-fidelity CFD simulations in real-time to rapidly iden-
tify and retrieve aerodynamic properties such as drag coefficient,
pressure distribution, and velocity fields.
We use the same approach employed for the Styling Agent,
utilizing a pre-trained CNN, specifically ResNet50 [40], to eval-
uate design diversity. Given a set of ğ‘images, each image ğ‘–is
represented by a feature vector fğ‘–âˆˆRğ‘‘. We then calculate the
diversity score by computing the average pairwise L2 distancebetween these feature vectors. Images exhibiting higher average
distances are considered more diverse, reflecting greater visual
or stylistic variance within the dataset. As shown in Figure 11,
the retrieval system identifies similar designs and provides direct
aerodynamic comparisons. Engineers can quantify the improve-
ments in aerodynamic drag and analyze the wake flow, gaining
insights into flow separation and turbulence characteristics. This
processbridgesthegapbetweenearly-stageconceptualdesignand
performance-driven engineering, enabling data-driven decisions
for optimizing aerodynamics while maintaining stylistic intent.
Since DrivAerNet++ includes multiple modalities for each
design, such as parametric data, point clouds, and part annota-
tions, this additional information can be leveraged for various
downstream tasks. For instance, parametric data can be utilized
for manufacturing processes and constraints, while point cloud
representations enable advanced shape analysis and reverse en-
gineering. Furthermore, part annotations provide structured in-
sightsintocomponent-levelperformance,facilitatingtargetedde-
sign optimizations and generative modeling for aerodynamic and
structural improvements.
3.4.2 Real-time Aerodynamic Predictions with Surro-
gate Modeling. In this work, we utilize TripNet [30], a tri-
plane neural architecture, within the Simulation Agent to rapidly
and accurately predict aerodynamic properties directly from 3D
car geometries. This enables the Simulation Agent to predict
aerodynamic properties such as drag coefficients, surface pres-
sure distributions, and full volumetric flow fields in real-time,
significantly accelerating aerodynamic analyses. The model is
trained on the DrivAerNet++ dataset and evaluated on an unseen
test set consisting of 1,200 industry-standard car designs, provid-
ing a comprehensive evaluation due to the extensive size of the
test set.
Figure12illustratesthedesigntrendsindragcoefficientanda
comparisonbetweengroundtruthandpredictedvaluesforvarious
car designs. The ground truth values are arranged in ascending
order of drag coefficient and plotted alongside the predicted val-
ues for the same designs. The model effectively captures the
overalltrend,withpredictionscloselyfollowingthegroundtruth.
However, the predicted values exhibit some oscillatory behavior,
particularly in smaller directional variations between successive
designs. This oscillation is acceptable and does not significantly
impact the overall trend, as the model effectively captures the
general pattern of drag coefficient variation.
Figure 12: Comparison of ground truth and predicted drag coefficient
trendsacrossdifferentcardesignsfromtheDrivAerNet++unseentestset,
whichincludes1200industry-standardcardesigns. Themodeleffectively
captures the overall trend, with acceptable oscillatory behavior in the
predictions.
Figure 13: Probability density distribution of drag coefficients for
groundtruth(CFDsimulations)andmodelpredictions. Thestrongover-
lapindicatesthatthemodelaccuratelyreplicatesthestatisticaldistribution
of the aerodynamic performance of the unseen test set.
Figure13presentstheprobabilitydensitydistributionofdrag
coefficient values for both the ground truth from CFD simula-
tions and the model predictions. The distributions closely align,
demonstratingthatthepredicteddragcoefficientsfollowasimilar
statistical pattern to the ground truth values. The strong overlap
between the two distributions indicates that the model effectively
captures the overall variability and range of aerodynamic perfor-mance across different car designs. Minor deviations in the pre-
dicted distribution, such as slight differences in peak density and
secondarymodes,suggestareasforpotentialrefinementbutdonot
significantly impact the overall agreement. This result highlights
themodelâ€™scapabilityinproducingrealisticaerodynamicpredic-
tions, reinforcing its applicability in engineering simulations.
Finally, the results in Table 3 demonstrate the simulation
agentâ€™s capability to accurately predict differences in drag coef-
ficients (Î”ğ¶ğ·) between various car configurations compared to
CFD simulations. For instance, the predicted drag difference
between the FD-FS and FD-N configurations is 0.066, closely
matchingtheCFDvalueof0.065. Similarly,thesimulationagent
correctly predicts the drag difference between the E-FS and E-N
configurations, with only minor deviations of 0.001. The largest
discrepancy appears in the N-FS configuration, where the pre-
dicted value is -0.00007 compared to the CFD result of -0.0003,
though this remains within an acceptable range.
CFD Simulation Simulation Agent
FD-FS 0.065 0.066
FD-N 0.065 0.066
E - FDâˆ’0.037âˆ’0.038
N - FSâˆ’0.0003âˆ’0.00007
E - FS 0.028 0.028
E - N 0.029 0.028
Table 3: Comparison of differences in drag coefficients ( Î”ğ¶ğ·) from
CFD simulations and the Simulation Agent (TripNet). Results are com-
putedonanunseentestsetcomprising1,200diversecardesignsfromthe
DrivAerNet++ dataset, covering estateback (E), notchback (N), fastback
smooth underbody (FS), and fastback detailed underbody (FD) configu-
rations.
4. RESULTS AND OBSERVATIONS
Our method enables AI-assisted rapid iteration in car design
bygeneratingaestheticvariationswhilepreservingtheunderlying
structural integrity of the cars. The application of LLMs and
VLMs in stylization introduces creative diversity, making this
technique suitable for industrial applications such as automotive
concept design and custom styling.
Togeneratehigh-qualitystylizedcardesigns,weutilizeStable
Diffusion XL (SDXL) [11], a latent diffusion model capable of
producing photorealistic and artistically enhanced images based
ontextprompts. SDXLleveragesadual-stagearchitecturewitha
basemodelforcoarsegenerationandarefinementmodelforhigh-
detailenhancements. ThemodelisconditionedusingControlNet,
allowingustoguidethegenerationprocesswithCannyedgemaps,
ensuring that the output images maintain the structural integrity
of the original car designs. This approach allows for precise
control over styling variations, enabling the synthesis of diverse
and visually appealing automotive concepts.
The results primarily summarize the workflow outlined in
Section3. Beginningwitha2Dsketchofanestatebackcardesign,
the Styling Agent generated high-resolution rendered images, as
showninFigure7. Next,theCADAgentretrievedthemostsimilar
designs from the DrivAerNet++ database and further generated
new3Dcarshapesbyinterpolatingbetweentheestatebackdesign
and three other configurations (Figure 9). The Meshing Agent
then processed the generated 3D models, producing high-quality
CFD meshes optimized for aerodynamic simulations. Finally,
the Simulation Agent performed real-time aerodynamic analysis
by retrieving the most similar designs from the DrivAerNet++
database and obtaining corresponding aerodynamic performance
data, as illustrated in Figure 11.
5. IMPLICATIONS FOR DESIGN
The integration of AI-driven Design Agents into automotive
designistransformingthetraditionalworkflowbyautomatingkey
taskssuchasconceptgeneration,performanceevaluation,andde-
sign optimization. Our multi-agent system leverages generative
models,geometricdeeplearning,andhigh-fidelitysimulationsto
enable engineers and designers to explore a vast design space
efficiently. By orchestrating interactions between specialized
agentsâ€”ranging from styling and CAD retrieval to meshing and
simulationâ€”the framework accelerates the iterative design pro-
cesswhilemaintainingindustry-standardengineeringconstraints.
As these AI-powered agents continue to evolve, their ability
to collaborate with human designers will redefine the future of
automotive engineering. Rather than replacing human intuition,
these agents serve as intelligent assistants, augmenting creativity
and providing real-time feedback on aerodynamic performance
and manufacturability. This approach not only streamlines the
developmentpipelinebutalsoensuresthatnoveldesignsareboth
aesthetically compelling and functionally optimized. While our
framework specifically addresses automotive design, the impli-
cations of AI-driven design agents extend broadly to any engi-
neering domain involving performance-driven simulations, such
as aerospace, naval architecture, or structural engineering. Ulti-
mately, this approach fosters a more interactive and iterative de-
sign environment, bridging creativity, aesthetics, and functional
requirements, and reshaping traditional design methodologies by
significantly enhancingefficiency andcreativity throughcollabo-
rative human-AI workflows.
6. LIMITATIONS, FUTURE WORK, AND CONCLUSION
One limitation of our approach is that mesh evaluation cur-
rentlyfocusesprimarilyongeometricandtopologicalqualitymet-
rics obtained from utilities like checkMesh . However, accurate
mesh evaluation should also incorporate validation through CFD
simulation outcomes, experimental data, and adherence to estab-
lished best practices. Additionally, aesthetics evaluation can be
inherently subjective; thus, future work should include surveys
with engineers, designers, and potential users to quantitatively
assess the visual appeal and practicality of generated designs.
Anotheravenueforfutureworkinvolvesexploringalternative
multi-agent orchestration frameworks beyond AutoGen to assess
whether different systems could further enhance agent collabora-
tion,scalability,ortaskmanagementeffectiveness. Finally,while
our approach focuses on early-stage conceptual design, styling,
andaerodynamicevaluation,futureworkcouldextendthesecapa-
bilities to prototyping, manufacturing, and market-driven design
analysis.
Finally, we conclude by presenting an AI-driven multi-
agentframeworkleveragingstate-of-the-artvision-languagemod-
els (VLMs), large language models (LLMs), and geometric deep
learningmethodstosupportandacceleratekeystagesoftheauto-motivedesignprocess. Ourapproachuniquelyintegratesaesthet-
icsandaerodynamicperformanceusingahigh-fidelity,large-scale
dataset comprising 8,000 industry-standard designs. We demon-
strated how this multi-agent system facilitates seamless interac-
tionamongdesigners,engineers,deeplearningandgenerativeAI
models,aswellasengineeringsoftwaresuchasOpenFOAM,Par-
aView,andBlender. Theframeworkenablesreal-timeretrievalof
CADmodels,parametricrepresentations,orCFDsimulationdata
based on geometric and performance similarity. Additionally,
intuitive text prompts streamline styling and meshing tasks, al-
lowingnewlygenerateddesignstoberapidlymeshedorevaluated
for aerodynamic performance through surrogate models. These
capabilities highlight the transformative potential of integrated
AI-driven methods in engineering design workflows, encourag-
ing further research and practical adoption across academia and
industry.
REFERENCES
[1] Forslund, Karin and Soderberg, Rikard. â€œAesthetic conse-
quences of making car exteriors visually robust to geomet-
rical variation.â€ Journal of Design Research Vol. 8 No. 3
(2010): pp. 252â€“271.
[2] Li,YanlongandZhu,Hui. â€œAresearchonelectriccarstyling
designandlowaerodynamicdrag.â€ IOPConferenceSeries:
MaterialsScienceandEngineering ,Vol.573. 1: p.012014.
2019. IOP Publishing.
[3] Li, Krista J and Liu, Yan. â€œSame or different? An aesthetic
design question.â€ Production and Operations Management
Vol. 28 No. 6 (2019): pp. 1465â€“1485.
[4] Catalano, Chiara E, Giannini, Franca, Monti, Marina and
Ucelli,Giuliana. â€œAframeworkfortheautomaticannotation
of car aesthetics.â€ AI EDAM Vol. 21 No. 1 (2007): pp.
73â€“90.
[5] Cheutet, Vincent. â€œ2D semantic sketcher for a car aesthetic
design.â€Proc. CPI Vol. 18 (2007): pp. 1â€“14.
[6] Cheutet, Vincent, Catalano, Chiara Eva, Pernot, Jean-
Philippe, Falcidieno, Bianca, Giannini, Franca and LÃ©on,
Jean-Claude. â€œ3D sketching for aesthetic design using fully
free-form deformation features.â€ Computers & Graphics
Vol. 29 No. 6 (2005): pp. 916â€“930.
[7] Jagtap, Santosh and Jagtap, Sachin. â€œAesthetic design
process: Descriptive design research and ways forward.â€
ICoRDâ€™15â€“Research into Design Across Boundaries Vol-
ume 1: Theory, Research Methodology, Aesthetics, Human
Factors and Education : pp. 375â€“385. 2015. Springer.
[8] Yadav, HC, Jain, Rajeev, Shukla, Sandarbh, Avikal, Shwe-
tankandMishra,PK.â€œPrioritizationofaestheticattributesof
carprofile.â€ InternationalJournalofIndustrialErgonomics
Vol. 43 No. 4 (2013): pp. 296â€“303.
[9] Liu, Yan, Li, Krista J, Chen, Haipeng and Balachander,
Subramanian. â€œThe effects of productsâ€™ aesthetic design
on demand and marketing-mix effectiveness: The role of
segment prototypicality and brand consistency.â€ Journal of
Marketing Vol. 81 No. 1 (2017): pp. 83â€“102.
[10] Burnap,Alex,Hauser,JohnRandTimoshenko,Artem.â€œDe-
signandevaluationofproductaesthetics: Ahuman-machine
hybrid approach.â€ Available at SSRN 3421771 (2021).
[11] Podell, Dustin, English, Zion, Lacey, Kyle, Blattmann,
Andreas, Dockhorn, Tim, MÃ¼ller, Jonas, Penna, Joe and
Rombach, Robin. â€œSdxl: Improving latent diffusion mod-
els for high-resolution image synthesis.â€ arXiv preprint
arXiv:2307.01952 (2023).
[12] Zhang,Lvmin,Rao,AnyiandAgrawala,Maneesh. â€œAdding
conditionalcontroltotext-to-imagediffusionmodels.â€ Pro-
ceedingsoftheIEEE/CVFinternationalconferenceoncom-
puter vision : pp. 3836â€“3847. 2023.
[13] Elrefaie, Mohamed, Dai, Angela and Ahmed, Faez. â€œDri-
vaernet: A parametric car dataset for data-driven aero-
dynamic design and graph-based drag prediction.â€ In-
ternational Design Engineering Technical Conferences
and Computers and Information in Engineering Con-
ference. arXiv preprint arXiv:2403.08055. 2024. DOI
10.1115/DETC2024-143593. URL https://doi.org/10.1115/
DETC2024-143593.
[14] Elrefaie,Mohamed,Morar,Florin,Dai,AngelaandAhmed,
Faez. â€œDrivAerNet++: A Large-Scale Multimodal Car
Dataset with Computational Fluid Dynamics Simulations
and Deep Learning Benchmarks.â€ Advances in Neural In-
formation Processing Systems Vol. 37 (2025).
[15] Greenshields, Christopher. OpenFOAM v11 User Guide .
The OpenFOAM Foundation, London, UK (2023). URL
https://doc.cfd.direct/openfoam/user-guide-v11.
[16] VanderMaaten,LaurensandHinton,Geoffrey.â€œVisualizing
data using t-SNE.â€ Journal of machine learning research
Vol. 9 No. 11 (2008).
[17] Wu, Qingyun, Bansal, Gagan, Zhang, Jieyu, Wu, Yiran, Li,
Beibin, Zhu, Erkang, Jiang, Li, Zhang, Xiaoyun, Zhang,
Shaokun, Liu, Jiale et al. â€œAutogen: Enabling next-gen llm
applications via multi-agent conversation.â€ arXiv preprint
arXiv:2308.08155 (2023).
[18] Community,BlenderOnline. Blender-a3Dmodellingand
rendering package . Blender Foundation, Stichting Blender
Foundation, Amsterdam (2018). URL http://www.blender.
org.
[19] Ahrens, James, Geveci, Berk, Law, Charles, Hansen, C,
Johnson, C et al. â€œ36-paraview: An end-user tool for large-
data visualization.â€ The visualization handbook Vol. 717
(2005): pp. 50038â€“1.
[20] Demirel,HOnan,Goldstein,MollyH,Li,XingangandSha,
Zhenghui. â€œHuman-centered generative design framework:
An early design framework to support concept creation and
evaluation.â€ InternationalJournalofHumanâ€“ComputerIn-
teraction Vol. 40 No. 4 (2024): pp. 933â€“944.
[21] Yuan, Bingkun, Wu, Kai, Wu, Xinying and Yang, Chaox-
iang. â€œForm generative approach for front face design of
electric vehicle under female aesthetic preferences.â€ Ad-
vanced Engineering Informatics Vol. 62 (2024): p. 102571.
[22] Ananthan, Vidyasagar, Ashton, Neil, Chadwick, Nate,
Lizarraga, Mariano, Maddix, Danielle, Maheswaran,
Satheesh, Moreno, Pablo Hermoso, Shabestari, Parisa M,
Sovani, Sandeep, Subramanian, Shreyas et al. â€œMachine
learning for road vehicle aerodynamics.â€ Technical report
no. SAE Technical Paper. 2024.[23] Arechiga, Nikos, Permenter, Frank, Song, Binyang and
Yuan, Chenyang. â€œDrag-guided diffusion models for ve-
hicle image generation.â€ arXiv preprint arXiv:2306.09935
(2023).
[24] Radhakrishnan, Sreedhar, Bharadwaj, Varun, Manjunath,
Varun and Srinath, Ramamoorthy. â€œCreative intelligenceâ€“
automatingcardesignstudiowithgenerativeadversarialnet-
works(GAN).â€ InternationalCross-DomainConferencefor
MachineLearningandKnowledgeExtraction : pp.160â€“175.
2018. Springer.
[25] Edwards, Kristen M, Man, Brandon and Ahmed, Faez.
â€œSketch2Prototype: rapidconceptualdesignexplorationand
prototypingwithgenerativeAI.â€ ProceedingsoftheDesign
SocietyVol. 4 (2024): pp. 1989â€“1998.
[26] Ocker, Felix, Menzel, Stefan, Sadik, Ahmed and Rios, Thi-
ago. â€œFromIdeatoCAD:ALanguageModel-DrivenMulti-
Agent System for Collaborative Design.â€ arXiv preprint
arXiv:2503.04417 (2025).
[27] Park, Jeong Joon, Florence, Peter, Straub, Julian, New-
combe,RichardandLovegrove,Steven.â€œDeepsdf: Learning
continuous signed distance functions for shape representa-
tion.â€ProceedingsoftheIEEE/CVFconferenceoncomputer
vision and pattern recognition : pp. 165â€“174. 2019.
[28] Qi, Charles R, Su, Hao, Mo, Kaichun and Guibas,
Leonidas J. â€œPointnet: Deep learning on point sets for 3d
classification and segmentation.â€ Proceedings of the IEEE
conference on computer vision and pattern recognition : pp.
652â€“660. 2017.
[29] Wang, Yue, Sun, Yongbin, Liu, Ziwei, Sarma, Sanjay E,
Bronstein, Michael M and Solomon, Justin M. â€œDynamic
graph cnn for learning on point clouds.â€ ACM Transactions
on Graphics (tog) Vol. 38 No. 5 (2019): pp. 1â€“12.
[30] Chen, Qian, Elrefaie, Mohamed, Dai, Angela and Ahmed,
Faez. â€œTripNet: Learning Large-scale High-fidelity 3D
CarAerodynamicswithTriplaneNetworks.â€ arXivpreprint
arXiv:2503.17400 (2025).
[31] Xi, Zhiheng, Chen, Wenxiang, Guo, Xin, He, Wei, Ding,
Yiwen, Hong, Boyang, Zhang, Ming, Wang, Junzhe, Jin,
Senjie, Zhou, Enyu et al. â€œThe rise and potential of large
language model based agents: A survey.â€ Science China
Information Sciences Vol. 68 No. 2 (2025): p. 121101.
[32] Canny, John. â€œA Computational Approach to Edge Detec-
tion.â€IEEE Transactions on Pattern Analysis and Machine
Intelligence Vol. 8 No. 6 (1986): pp. 679â€“698.
[33] Vinker, Yael, Pajouheshgar, Ehsan, Aberman, Kfir, Cohen-
Or, Daniel, Lischinski, Dani and Bermano, Amit H. â€œCLI-
Passo: Semantically-AwareObjectSketching.â€ Proceedings
of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition : pp. 22147â€“22156. 2022.
[34] Radford, Alec, Kim, Jong Wook, Hallacy, Chris, Ramesh,
Aditya, Goh, Gabriel, Agarwal, Sandhini, Sastry, Girish,
Askell, Amanda, Mishkin, Pamela, Clark, Jack, Krueger,
Gretchen and Sutskever, Ilya. â€œLearning transferable visual
models from natural language supervision.â€ International
Conference on Machine Learning : pp. 8748â€“8763. 2021.
PMLR.
[35] Frans, Kevin, Cheng, Chin-Yi, Fang, Ziqi, Gao, Yitong,
Nguyen, Jason Yim, Qian, Lei, Zhang, Feng and Hamilton,
William.â€œCLIPDraw: ExploringText-to-DrawingSynthesis
via Language-Image Encoders.â€ (2021). URL 2106.14843.
[36] Rombach, Robin, Blattmann, Andreas, Lorenz, Dominik,
Esser, Patrick and Ommer, BjÃ¶rn. â€œHigh-Resolution Im-
age Synthesis with Latent Diffusion Models.â€ Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition (2022).
[37] Morita, Hayata, Shintani, Kohei, Yuan, Chenyang and Per-
menter, Frank. â€œVehicleSDF: A 3D generative model for
constrained engineering design via surrogate modeling.â€
arXiv preprint arXiv:2410.18986 (2024).
[38] Miao, JiangDong, Ikeda, Tatsuya, Raytchev, Bisser, Mi-
zoguchi, Ryota, Hiraoka, Takenori, Nakashima, Takuji,
Shimizu,Keigo,Higaki,ToruandKaneda,Kazufumi. â€œMa-
nipulating Vehicle 3D Shapes through Latent Space Edit-
ing.â€arXiv preprint arXiv:2410.23931 (2024).
[39] Lorensen,WilliamEandCline,HarveyE.â€œMarchingcubes:
Ahighresolution3Dsurfaceconstructionalgorithm.â€ Semi-
nalgraphics: pioneeringeffortsthatshapedthefield (1998):
pp. 347â€“353.[40] He,Kaiming,Zhang,Xiangyu,Ren,ShaoqingandSun,Jian.
â€œDeepresiduallearningforimagerecognition.â€ Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition : pp. 770â€“778. 2016.
[41] Pandey, Sandeep, Xu, Ran, Wang, Wenkang and Chu, Xu.
â€œOpenFOAMGPT: A retrieval-augmented large language
model (LLM) agent for OpenFOAM-based computational
fluid dynamics.â€ Physics of Fluids Vol. 37 No. 3 (2025).
[42] Xu, Leidong, Mohaddes, Danyal and Wang, Yi. â€œLLM
Agent for Fire Dynamics Simulations.â€ arXiv preprint
arXiv:2412.17146 (2024).
[43] Chen, Yuxuan, Zhu, Xu, Zhou, Hua and Ren, Zhuyin.
â€œMetaOpenFOAM: an LLM-based multi-agent framework
for CFD.â€ arXiv preprint arXiv:2407.21320 (2024).
[44] OpenFOAM Foundation. Meshing with snappyHexMesh
(2023). URL https://www.openfoam.com/documentation/
guides/latest/doc/guide-meshing-snappyhexmesh.html. Ac-
cessed: 2024-06-05.
[45] OpenAI. â€œGPT-3.5 Turbo.â€ https://platform.openai.com/
docs/models/gpt-3-5-turbo (2023). Last Accessed: 2024-
03-16.